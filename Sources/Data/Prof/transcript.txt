 All right. So also before we start, as you know, we have on, oh, let me first show my screen. So before we start, remember, we have those participation grades, okay, and part of the participation is basically the exercises you did two weeks ago. And then also the work on the adult income data set that you did last week. So make sure you upload it here on Moodle, okay. So I think many of you, I checked on Friday and there were only like 20 30 and now it's almost 60. So many of you seem to work on the weekends. But if still someone, you know, hasn't submitted it, make sure you submit it here on Moodle. So I will check it in the end. So if you need two more days, that's fine. Or a little bit longer, but I will check it at the end, you know, of the semester, who submitted something on hard. And it's really not about how you did it. It's just that you participate it, okay. So also for the people, for example, who use chat, chat GPT, then that is fine. You can also upload your, your checks with chat GPT, you know, as a screenshot, also as a link. So I can see what you, what you use prompts, that is fine too. Okay, I just want to see that you worked on the data set. Are there any questions? So in general any comments or anything you would like to talk about before we start. Now today, as I mentioned before, as you can see in this schedule, I would like to talk about generative AI. So the entire blog today is about generative AI, especially large language models. And especially here, chat GPT, okay. We also look into some, some other models. You will see that the performance is not, not, not as well as, uh, chat GPT. There are models that are for free, that are, you know, public that you can basically download and use on your local machine. And we're going to take a look into that to a little bit, okay. Also you can see next week. So I'm not really doing a class next week. But I would like you to submit your project selection or proposal. So this, this, this, of today, I will introduce you to a set of projects that I have in mind for you. That you could work on in the domain of generative AI. And I would like you to select either select a project from that, from that selection or. If you have your own ideas, that's fine too. Okay, we can talk about it. What I would like you to do is basically spend the next week thinking about a project and submit it by, and often, you know, by, by next Monday, the latest. Okay, so we can, we can discuss about the ideas. We can also discuss about how you, you know, if you select a project from the selection that I have, if you have a certain, you know, preference or your technical questions, we can discuss about. Okay, I've seen the chat people are still waiting in the, let's see, oh yeah. Also in groups again, yes, you can do it in groups. Typically, I ask for student teams of three, you know, but if you have a team of four or two, let's find two. Having said that, I expect you to, you know, if you're a group of four students, of course, I expect you to do a little bit more than a group of two. Okay, so in the end, you grades, which is basically around the technical death that you that you accomplished in your project will be also, you know, based on your group size. So the group of four has to, you know, work harder more than than than a group of two, you know, so the outcome should be twice as much as a group of two. Any questions regarding that. So again, by next week, you know, think about the proposal, think maybe about questions, maybe think already about how would you accomplish this project. Is it kind of technically clear to you, you know, how to start and all these things we can discuss also you can write emails, okay, if you have some ideas by middle of the week, I'm online this week and also next week. If you have any questions, write an email and I will try to get back to you as soon as I can. Okay, so don't wait for the review session, as you can see here in two weeks, we're going to start with the review section, so I'm going to meet per team, you know, about 20, 30 minutes. And we just discuss about your project about your progress. But if you have anything before those meetings, you know, just write an email, that's fine, we can also set up a Skype zoom meeting and meet before those review sessions. Also, if you feel like you don't need to meet, you know, because you're you have a plan and you know what to do, then you don't need to show up on those review sessions. So it's just an option for you, it's, you know, you can use me again as a resource for your project and, and you know, it is your choice, you know, how you use them. Any questions and again, I will talk about the projects, you know, in this in this session yet. But just organizational wise, are there any questions still. I've got a question on the regular sessions going to be online or the other. Not typically, typically it's in person, but you know, it's for me, it's fine. You know, I can do it online, I can do it in person. Sometimes it's better to meet in person, if for example, if you have any technical issues, you know, so we can go together. It's easier to do it, you know, in person. Other things, you know, we can also just discuss online. So that's that's both is fine for me, you know, whatever works best for you. And we'll see maybe I'm not sure if I do those sessions in my office or in the classroom, I will let you know where, where we meet, if you want to meet in person. Any other questions. Okay. All right. So then let's join. Okay. So let me move this a little bit away. Do you see my chat and my participants, participants window here. Why is it just me? Is zoom windows here. I'm not sure if you see those or if you see my full screen without the windows can many, maybe someone. Let me know quickly. So right now, always seeing is the PowerPoint presentation. Okay. Okay. And you don't see the chat. Okay. So here's what we're going to do. Okay. So in general, you know, generative AI is based on deep neural networks. Okay. It's basically large neural networks, you know, with lots of hierarchies. And how they exactly work, you know, it's not part of this class. This is part of next year semester for the ones who are interested in that in the technical details. This is next semester, I'm offering this class applications of AI. So I'm going to consider it until again. It's an optional class. It's, you know, so you can take it. You don't have to. But this is where we go deep into, you know, how actually GPT and similar models work. Okay. So you learn how to, you know, how new networks work and how you train them and how to use them. You know, how certain architectures of new networks, like convolutional neural networks and sequential neural networks and then transform architectures, transform architecture is basically what chat GPT is built upon. And you know, you will understand how it's trained and so on. And this is more, more technical, more mathematical. So it's not really for everybody, you know, but if you're interested in learning how it actually works, then this is this is for you. And this class, we just look at the, we just look at how we use a generative AI and chat GPT and these sorts of models. And I'm not just talking about, you know, typing in prompts and getting an output. You would see this is more to it. And this is what I'm going to cover in this in this session. Okay. And you would see that I will mainly also talk about one paper that's that just came out this summer from Stanford. And this group around AI Stanford always had like a very strong AI group. And this group.  published this paper where they really did cutting edge prompt engineering. This is what I want to look into with you together so that you can see and understand in which direction this entire prompt engineering is going. You would also see they also have everything public, so the entire repositories on GitHub, you can take a look at it and you would see it's a lot of code in the end. It's not just prompt engineering, just a lot of Python code where they trigger and use a lot of different prompts based on certain prompt templates that they trigger based on certain events. How all this comes together, this is basically what I'm going to cover today. Based on this, I also have some suggestions for similar projects, not as complex. The Stanford teams, what was like 9 PhD students, I think they spend about 10,000 euros for prompt costs for OpenAI because it's not for free. I don't expect the project of that scale, of course, but I expect something along those lines. Again, I will talk about what exactly that is later. We're going to start very basic. In the beginning, I would like to take a look at you at ChatGPT, as you know it, the, basically, when we're in here, we're in to move it here. What I want to take a look at is the ChatGPT UI just to get and understand where we are with it. I think some of you have ChatGPT plus some not, so I just want to show you a little bit how ChatGPT plus works right now. What we have here in ChatGPT plus, so this is basically the GPT 4 version, and you have here some better functionality. It's all in better status. It's not really completely set up yet, but it's all better. If you sign up for GPT plus, it's $20 per month. You have access to those, but again, it's in better status. So one thing that I already showed you is this advanced data analysis. So if you go there, you basically have the option to upload a data set and to have ChatGPT work on a data set. So I think some of you did it. I also did it here with the, with the added income data set. So you basically, you know what you did in the last session, I just did it with, with ChatGPT. Is there any question? So you can just upload the zip file here and have ChatGPT analyze it. Okay, so this is what I did here with advanced data analysis. You just uploaded, you only had a prompt, unzip, clean, where data, where necessary, and you're in your features, build a prediction model, and present. Okay, so basically what you had to do last week, I asked ChatGPT to do, and it goes on, and does basically, as you can see here, generates Python code. So first it unzips the files, and then does everything, it takes a look at the data. I think it also unzips the other data files, it gets some first insights here. Then I think what's the structure, so it looks at all the variables, it always generates a summary from the Python code, as you can see here, then it starts with data cleaning, missing data, duplicate rows, and so on. And again, it generates a summary of what it found. I think it cleans a little bit, drop duplicates, data clean, and so, okay, so it's pretty good, you know, in terms of the doing, and I think in the first session I showed you already that there might be mistakes, so still you need to do some supervision here. But it does the work quite well, and it's quite compelling, you know, if you think it's just basically a model that generates prompts, it does everything pretty, pretty nicely, as you can see here, including categorical variables, the features scaling. I think we talked about class imbalance, it also detects, that's for example, the target variable and the income data set is imbalance, so there are more people with an income less than 50,000 than above 50,000, so it detects that, and you know, it suggests maybe how to deal with that, and then it generates a model basically, it calculates a accuracy, 86, 86%, and so on, okay, so it's pretty good, you know, this is basically what you do with data analysis. Also, this is how I generate basically all the slides with, for example, in the intro, you know, this year, as I mentioned before, it's all with a data analysis tool, you know, that I basically said, okay, just, you know, take a linear model and fit some data points, this graph is basically from ChatGPT, it's based on Python code, you know, it's basically, map.lib and C-borne libraries that it generates, but it generates it for me, I don't need to code it, you know, so he asked it to generate a piecewise function, he asked it to fit a decision tree on it, then, you know, decision tree of level three, decision tree of level 10, so it's all basically generated by ChatGPT, you know, sample test data, so it added test data points to the graphs, and so on. So this is all that work, so it really helps a lot for every data scientist, you know, this is a tool that just helps a lot in your daily work. And now, what else do we have in beta? As you can see here, you have the default version, the default version now has since, I think, two weeks or so, has here something where you can attach an image, okay, so you can attach an image here, or if you use your phone, you also have apt, GPT, app for Android and everything, so you can just take your phone, take a picture, and submit it to ChatGPT, so what I did, for example, is I gave ChatGPT here, you know, I just submitted a slide of mine, you remember this slide here on top of the UI, that's, you know, one of the intro slides I gave in the first session, so I asked basically ChatGPT, okay, look at that slide, can you generate a script for that slide? Because you know my slides are not very self-speaking, they're not very, you know, I have my own style, let's put it this way, but you know, as you can see, ChatGPT basically understands what I wanted to say, you know, basically the evolution of data science, you know, how we did before, you know, using tools and stuff like this, and it always tells them the narrator here, you know, what to say, you know, and where to point and think somewhere, it says okay, now you should point to the other, I hear point to the other possibility sections, move to the data science after side, so it's, you know, it's basically guiding, you know, someone through the slide where to look and what to read and what to think about it, so, and it gets it, you know, it gets it basically what I wanted to say on that slide, and that's pretty amazing too, okay, so you can, ChatGPT can see now, and you can use it, another example is here, what do I have here, so I just submitted a nice UI I found on the web, and I asked okay, can you implement this in React, okay, so React is this ChatGPT framework, and it does it, you know, certainly, you know, it generates it, it doesn't fill it, you know, all the components are not really filled with back and, you know, data and stuff, but the front end is basically the, the mask, it, you know, it doesn't, you know, so basically you have the structure, the basic structure, and then you would also like a CSS chart here, and now you would start to fill it, you know, with images or whatever you have, but you can also ask for help, you know, just ask, okay, you know, take the main class or the class name cards, can you fill it, you know, with code that, represents a chart or whatever, it will do, okay, so this is really, you know, in my opinion, it's pretty impressive, and you see, you know, everywhere on Twitter, and so you need, you see very nice use cases for all of these functionalities, you know, and this is also what I encourage you to do, to follow, you know, go on Twitter and, you know, because it's on a daily basis, you get new things that people do with CHEDGPT, and you want to be, you know, up to date, and this is also what companies are doing, I mean, this is money, if you have something like this, the way we code will change drastically, you know, the way you learn how to code in the first semester of your bachelor's studies, will change drastically, you know, we are all aware of it, and companies do it already, because it's, you know, lots of money that they can save, you know, if you have a developer team of 10 people, and someone using, you know, co-pilot and combination with CHEDGPT4, you know, you can reduce your staff probably by a, by a lot extent, you know, you still need supervision, but the way how we code is different, we don't, you know,  code everything by ourselves. We let it code and just do code review and supervise. So again, this will change drastically and this is why I think it's very important for you to be aware of that and to acquire those kind of skills. How do you use AI for coding? And they call it AI, it's coding and this is basically the skills you need to acquire. Because it's not in five years, it's starting right now. Are there any questions? Again, if you have any comments also, if you would like to discuss, so I have a different opinion that's fine, we can discuss. It's basically looking into the future and the future is changing right now. And quite frankly, in my lifetime, I haven't seen more advanced technological progress. So I think it's really my stone in all our kind of lives. Okay, what else do we have? So we are talking about, so what I recommend is really to go on Twitter, you know, sign up and you don't need to tweet, but follow the right people. If you start with following OpenAI and you always get suggestions to follow else, you are up to date and you see basically what people are doing. And again, it's on a daily basis, with this image, people are starting, taking pictures of their refrigerators and let chat GPT generate recipes based on what they have in the refrigerator. And it does it. And then I will show you more. You can also order the food and generate shopping lists from there. Another guy showed how to fix his bike. I had a broken bike, so he took a picture of his bike and GPT told him, this is broken, you can fix it the following way. So it basically gave a manual, how to fix his bike, what kind of wrench you need and so on. So pretty nice use cases. So the other thing I wanted to show you is basically, yeah, so what you also have here, so as default, you get now this image upload here, you have advanced data analysis, you also have browsing. So if you are all aware that chat GPT is trained based on data up to I think September 2021. Now if you have anything you're interested that's after 2021, you need to tap into the web and GPT Plus can do it too. So what did I have here? Yeah, this nice question. So I think it was on Friday, so maybe I can type it again. So I'm basically asking who's Leonardo DiCaprio's girlfriend because it changes a lot. So you don't know if it's the same as in 2021, so you have to go to the web. And then I ask what is the current age race for the power of 0.43? You will later see why I asked that. So now you can see when I do it, it starts browsing. Chat GPT understands, this is data I don't have in my training. So it starts browsing, you know, generally look at different websites, getting a verification, so chat GPT knows how to browse and basically getting that information out there. Okay, so let's wait quickly. There's you can see it's looking at different web pages, you know, you saw Wikipedia, you saw another page, so it's taking different web pages to make sure you get the right information. And as you can see also, it gives you the links where it got it from. Okay, so these are all the sources from all the web pages, so you can just go to the web pages and read it by yourself. Okay, now you can see here, it found Victoria Serretti and she's 25. Now it calculates the age to the power and gives you the answer. Later we'll see this is not correct. Okay, this is something I want to discuss with you later. Okay, now so this is web browsing, you can ask for the weather, what is the weather, and then so I can't do it start browsing because this just data knows that it's not in the in the training data up to 21. So we do it. Why we wait for for GPT, I have a question. Yes, of course. I would personally consider programming and delegating chat GPT into programming to be two different skills. I don't know if you agree with that or not, and my question would be would you consider those two skills to be essentially equal to one another? So if you had two people who you were considering hiring one person says they they program on their own, they build everything on their own, they design the architecture on their own, and the other person says they know how to prompt GPT into doing it for them. Do you think that that would be sort of equal in terms of skills? No, right now you need to understand how to program. So I wouldn't you know in terms of what you asked me in terms of which person I would took. I would also agree with you, take the person who did you know in more herself before. But I would I would if I would hire you as a as a as a strong programmer, I would expect you to use those tools. In order to to make you to work fast. That's that's what I would basically you know ask you. For example, for example, a lot of those code is also hired for code review and code reviews taken a long time. But using those tools you can I just have a number from from two or three companies, but they talked about you know having code reviews from their programmers 10 times faster than before. So and if you would be someone you know I do my code review, but I do it my all the way. I don't use use those tools. Then I would say well you know you probably slower than your teammates were using those tools. Okay, it's just the the question just came up in my mind when when you mentioned that the students learning programming right now are also doing using chat GPT or learning to use chat GPT, which I don't know in my mind seems a little contradictory that you're trying to learn how to program while at the same time having something else program for you. Well, I think you know the programming like part one and part two as you as you as you need to to know you need to understand code right. If you don't understand code if you don't know how how pieces are fit together, well you just trusting GPT and as you could see in the advanced data analysis you cannot trust GPT at this point. But on the other hand for example take me you know I I was a data scientist you know for a long time and now I teach it. But as you can see my my slides and my analysis you know doing those those charts that I just showed you you know it's so much easier and faster to do those charts with GPT then without it. This is why I always used to draw it on the on the on the chart board because creating those charts you know by myself would have taken I don't know you know maybe a week and with GPT it took me it took me an afternoon most. I was telling GPT what to do but it generated the code from okay thank you. You're welcome. But it's a good good question and if you have more of those you know if you're free to we have we have enough time you know that's that's fine we can discuss about it because it's really something we also opinions drift apart you know also within our and our you know microleague basically you know we're we're having similar discussions. Also other people say okay we shouldn't enforce students to use GPT because somehow certain skills getting getting kind of you know students are learning not to think themselves anymore but just using GPT and that's not really a full skill and you know they're they're some some truth to it you know and on the other hand you know in my opinion we you know if we have a lighter we use a lighter to make fire we don't you know you know push stones you know or you know something like this in order to make fire so in my opinion we always should use tools you know we have at our hand but but you know we should make sure we understand it and at this point we need to understand how which GPT is and how to use it correctly so that the output is good but if it's matched faster than before why not use it okay and so this is really something we we we discuss too and they are different opinions to it there's probably no right or wrong and we have to be a little bit careful in you know how to use it and it's also in terms of what to teach it  for us is really the question. I mean, do we? And there was always a similar discussion, you know, in my case, I still learned how to program with with zeros and ones, you know. So this this assembler based coding, this is what I still learned, you know, was a value for me throughout my life. I'm not really sure. Quite frankly, I never used it. Again, was it maybe good to understand it? I don't know, maybe at some point, but I don't really, and so at some point, you just have to say, okay, okay, here we do the cut, you know, you don't need to understand zeros and ones and how electricity flows back and forth, you know. So we just, you know, leave that away and go one step ahead. Okay, and this is, I think, what we need to do now. Okay, we we get some new technology, we get to get familiar with that technology. So, you know, on the bottom, you know, some things we just probably can't skip right now. But again, that's just my my my view and it might not be the right one. Also, you know, I remember two years ago, I was discussing with someone, you know, about general purpose AI, you know, basically special purpose AI is basically AI built for a special purpose, to be driving driving a car by itself, you know, or playing alpha go by itself or playing chess, you know, it's very special purpose, you know, just hard coded to do that very well and in every setting. But general purpose is an AI, it's basically an AI that's developed and nobody really knows what you can do with it and how to use it. And I was discussing with someone and telling him, you know, two years ago, I never saw any, you know, real general purpose AI. It's just not there. And in my view, it was like 10 years ahead that, you know, maybe a general purpose AI comes along and there you go. You know, one year later, CzechGPD came along and this is my view now really the first general purpose AI because you can have so many use cases for it, you know, an open AI itself, you know, didn't really think about all these use cases. So this is really in my view, a general purpose AI. So I was completely wrong in terms of my timeline, you know, when we're going to see open AI. And then I think even open AI itself was not aware that what they were developing. I will show you later, you know, that the previous model GPT2, how that performed, you know, on GPT3 is just a bigger version of GPT2 and it's so much better. So I think they must have been blown away too by the performance of that model. But again, okay, so let's stick to the to what I've planned. So this is the browsing function, okay. Now another function that we can see here is the plugins, okay, plugins. It's basically you have here a plugin store, okay, you can go to the plugin store and now here, everybody basically can develop their own plugins, okay. And so you can see here the popular ones, you know, you can browse through and they're all different sorts of plugins. You can, for example, see Expedia, you're all no Expedia for booking, you know, travel hotels. So Expedia had a plugin or kayak, you know, searching flights. So they have already plugins, you know. And how do you use those? So if you installed some, so I, for example, installed Expedia and activated smart slides. Now when I ask GPT now, you know, can you book a nice hotel and say for me? So basically since those plugins are activated, you know, it's kind of detecting, okay, which plugin to use. And now since I'm looking for travel, it knows, okay, maybe I should use Expedia. You also take the, you know, look at the requests. So Cheshire PT is basically submitting to must say, okay, you know, this, this JSON file here and now it gets a JSON file back from Expedia is driving basically the findings. And now Cheshire PT summarizes those JSON files and also provides a nice file here for me to book. So it's basically an API that everybody can submit. And this I think also is quite quite a game changer, you know, if you think back about about the iPhone, you know, and back in 20 2007 when Steve Jobs announced the iPhone, it was a pretty cool phone, you know, for use guys, it was probably not, you know, you were too young, maybe I don't know. But you know, for everybody else, it was just like a complete game changer, how to use a phone and you know, what it could do. But 2007, the app store was not there. So nobody even thought about apps at that point, you know, just seeing the phone that you can use the internet and then make, you know, scroll and it was just nice that the touchpad was amazing. But the app store wasn't there. And I think the real game changer in the end was the app store because the entire world, every company, you know, has its own app. So for everything you can get apps. And this is what happens here with Chachy Petina. Now every company like Expedia, Kaya, they're of course interested, you know, getting in there with their plugin and being able to have people, you know, just using their phone, talking to your phone, hey, you know, it's all there. Can you book a flight for me? It's to must say in the desert, you know, it pulls the right plugins, you know, and gets the job done. So basically what you need in order to develop a plugin, I just played around a little bit, you just need to describe your endpoints in natural language. So what Expedia does is basically it has only endpoints, you know, of the interface to the databases and describes those endpoints in natural language. So basically, you know, if the user looks for a nice travel, you know, you can use this endpoint, it will provide you with these in those details, you know, what kind of arguments do I need and what kind of arguments do you get back? It's hard to put on those endpoints in Chachy Petina smart enough to use it, you know, whenever it needs. Okay. So another plugin I have here is smart slides. So if I want to, for example, can you generate a nice, a nice slides for machine learning and interaction. So basically, so to take now that that smart slides the way to go. So it goes to smart slides. And it already generates here four smart slides, kind of an outline, okay, for my for my slide deck, you know, so the first slide introduction to machine learning, the second slide, what is machine learning. So it sends this basically this is from from from GPT, the things about how it could look like and sends it to smart slides in order to generate the slides from me. Okay. So you can think of all these different plugins, you know, and again, this is kind of the the moment where where everybody got away as oh, there's a store for it and then entire word is starting to develop, you know, apps and plugins for for chat GPT. And the nice thing is, it's all in one you are, you know, you can just talk to your phone, whatever you want. And it has all all the access to it to everything that's digitally in the end accomplish, accomplishable. So I can hear down on my presentation quite frankly, smart slides. I'm not too compelled about that. If you look at this slide, it's not really, let's see, let's open it up. So here's his machine learning now, introduction to machine learning, what is machine learning, history, evolution, you know, in the end, well, you know, here, you know, what I told you also about supervised and supervised reinforcement learning, and common algorithms, future trends, well, you know, it's something it's not really my slide of my style of slides, but you know, it's just it's just the start. Okay, so that's that. Well, then also of course, Cheshire Patina has the only three, you all know that, you know, I'm not too excited about that, but can ask it to create a nice image with, let's say a realistic image with two cute cats and you know, those tools, you know, like mid-journey is one, so basically generates images for you. The thing what's not in here in the yet, let me just wait. That takes a little bit. There you go, that's cute. All right, so the thing what's what's not implemented yet is the combination. You can only use one plugin, one of those better features at a time. So for example, what I did here with the image, transfer, you know, to the picture of our lab.  And I just asked Chech if you're okay. What can it's German now? What can I improve with our lab? The first thing it shows is basically a cable management, also managing those cables. It's not adding any plans. You know, that's also, I also did it with my living room at home. You know, just can you recommend it? And of course it would be nice to have now an image generated by a dolly that shows how your lab could look like with plans and cable management and so on. But again, this is not combined. Once I select here, if I go to the default, I can submit an image as you can see here. But once I select dolly, the image goes away. So it's not combined yet. But in my view, yeah, in the end, it's a met of time. This is also possible. Now, this is the UI. This is actually not what I wanted to talk to you about. The main part is really how do we use Chech, GPT programmatically? Okay. So how do we use Chech GPT or OpenAI APIs and combine it with different things? We'll see later. What we can do programmatically at this point is, of course, we can use you know, prompts, get prompts and get answers. We can also use browsing programmatically. We can use any API in the end anyways, like with Google search or so. We can use, well, advanced data analysis. We can let GPT programmatically generate code for us. We can execute it in Python. So that's possible in the end too. We can use plugins. We can also have Chech GPT programmatically decide which API to use. It's called function calls. We'll show you later. And we can also generate API with images with an API. What we cannot do right now is really the image transfer. So with OpenAI, we cannot submit an image and have it analyzed. That's not possible at this point with OpenAI API. But anything else, it's basically possible to kind of do it programmatically. Okay. This is what I want to do with you. So let me jump now to my, to my pie charm. So I'm using the pie charm editor now. Not the, not the notebook anymore, but the pie charm. Just because it's easier for me to show you things here. Okay. And now, first thing is, how do we get to sign up? So if we use OpenAI within our Python code, we can import here a package from OpenAI, but what we need in order to use in any way is an API key. Okay. So you have to sign up here on Chech GPT to get a key. Okay. You can sign up with Google if you use Google. So let me just log in. And now, the most important thing now is that, of course, if you use OpenAI, it has some pricing. Okay. So let me open the pricing page. So if you use again, if you use Chech GPT UI with 3.5, the basic model, it's for free. If you want to use Chech GPT plus, it's $20 per month. Okay. I encourage you to do it. I think it's well invested money, but you don't need to. If you use OpenAI API, you don't pay a monthly fee, and you don't need to be a plus member, but you pay per token. This is how much it costs. Okay. The simple model, the model that is for free also in the UI is the 3.5. And this is what it costs. So basically, I have to move my windows. So it's 0.4, so they're different context models. If you use a model with larger context, so basically the context is your prompt size. Okay. If you have a 4K prompt size, it's basically 4,000 tokens in your context. You can also, within one prompt, you can also use a different model with 16,000 tokens in one prompt. Okay. It's a little bit more expensive. But you pay for 1,000 tokens, you pay 0.0015 cents as input. So if you submit as a prompt, you pay as an input. And the output that CHGPD generates is 0.002 cents for 1,000 tokens. So this one here, for example, you can think of tokens as pieces of words, with 1,000 tokens is about 750 words. So it's a little bit less. And this typically is a word, but if it's a long word, so for example, capabilities, it's maybe two tokens, you know, capability or so. So sometimes it's two tokens, one word. So this is why it's a little bit less words than tokens. And as you can see, this paragraph here is about 35 tokens. Okay. And GPT-4, the better model, you know, is a little bit more expensive. So of course, if you work with GPT and you want to test certain things out in the beginning, you know, I encourage you to use the free, you know, free, you know, you just use it for free test out, you know, certain prompts, you know, by just cutting paste into your CHGPD and, you know, testing, getting your first insights and how those prompts perform for free. So if they are good, you can, you know, use these models. And if you feel like, well, for this prompt, GPT-4 actually has much better reply, then you have a little bit more money in those tokens. In general, if you look at what the hand here, so if you look at this Harry Potter Chamber of Secrets, so that's the first Harry Potter book. It's a pretty big book. It's about a hundred thousand tokens, the entire book. So if you submit the entire book of Harry Potter, the first one, it would be two cents for you. Okay. So not really something I would say, you know, even as a student, you would say no, two cents I would say, okay, four, okay. So when you play with it later, I show a version, you can also use it for free, but I encourage you also to play around with GPT and, you know, you can, you can stay for a long time in the center area, and I think this is something everybody can afford. Make sure you set your limit. So if you work with it, this link here, let me just open it up. Let's leave it. So what you can do is in your platform here in OpenI, you can set limits. So you can set a heart limit right now, I have it at five dollars, okay. So once I reach five dollars per month on a monthly basis, it will just stop. It won't run any. You can also generate a specify a soft limit. It's one dollar. So once I beat one dollar, I will get an email, hey, look, you have done something here, you are about for dollar now, okay. So if you work with GPT, make sure you set those limits, okay, then there's no surprise for you in the end, okay. Set those limits otherwise, you know, you have maybe some wild loops that, you know, generates prongs on a, that have certain size, you know, in the hundreds of thousands, and suddenly your heart, you have a heart, you have a, you know, a big, big bell in your account. But as you can see, for example, I'm only approved for a hundred twenty dollar anyway. So even if I wanted, I cannot really go beyond a hundred twenty, I would have to request an increase here, maybe have to pay more. You can also look at your usage here. So for example, here you can see what I've done. So right now for this month, I'm at a 48 cents, okay. I'm not even sure what I did here. It also gives you, you know, specifies what the use is. You can see here, I used GPT for for eight cents, then GPT 3.5, instruct GPT, I'm not really sure what I did there. Embedding models, this is what I want to talk about with you a little bit later. But basically, you know, you see, it's, it's not, not, not a lot of money. Okay. Any questions so far?  Now, how do we use the API? Well, here's how you do it. Here's also a link to the description. You know, online, it's nicely described, and it's really no rocket science. Okay, this is basically what you do. You have this is basically your query here. The query is your prompt. Now, this is basically your prompt. And you typically specify a role in the API. And for chat GPT. Now, you would see later different roles. But here my role is the user and the content. So my prompt basically is who won the world series in 2020. Okay, this is my query. I put it here as a message. I specify my model here. Use GPT 3.5. It's not so expensive. So I just execute this part. Oh wait, I first have to import here my open AI key. So I saved it here in a configuration file. You can do it too. Just just just template here. You know, where you put in your keys that you. So I loaded that one. And now I load my query here. Execute the request. So now we call chat GPT. We got a response here. So that's basically how the response looks like. When you see that. So basically this is you get a JSON file back from from open AI. And this is basically how it looks like. And the content is what you get back. So you get a message back here. The role is assistant. So GPT is always the assistant. That's the role. And the content is basically those entries daughters. One of the words here is in 2020. Okay. Then you also have a finish reason here. Finish reason is stop. So it was just done. Next semester we will see there are different finish reasons depending on, you know, how the model performs. For example, if you go across the four or 16,000 contacts, you know, it could, you know, if you generate a two long of sequence. The finish reason might be the sequence is too long. So, you know, but typically it's a stop here. And it also the user, you know, tells us okay, you have 70 tokens you use completion token, 13 total total. So we gave in 17 tokens. GPT gave 13 tokens back. So it's all, you know, 1000 tokens is 0.00001 or two cents. This is basically an amount of money that's not really measurable anymore. You know, so, and this is typically what happens if you just use it in this way, they won't even charge you because it's below a cent. And okay, now, just to make sure that that's clear what happens. You send prompt by prompt. Okay. So what I do here this prompt now, if I would say, well, my message is now where was it played. It doesn't have any history. If you go to the UI and you enter a second question, it has a certain history, right? It will, it knows what it asked before here with those prompts. It doesn't. Okay. So if I asked this question here, where was it played. That's the response I get back. Okay. I apologize, but you haven't given any specific information, but what it refers. Okay. So it doesn't have the context that I was talking about, you know, the baseball games. Okay. So I would have to provide the context, you know, the answer in order to make sure it has all information would need to go all in the context here. Okay. So you would have to paste in here basically the response and also your query, you know, and then if I execute it now. Well, I have a project. Clearly, I have messages. Like that. Yeah. Now it now understands that a 2020 word series was played at Globe field in the early Texas. So it has the context because I gave basically my query, my original one, my response from chat GPT. And now what, what my next questions. And also you can see here, you can provide different roads. Okay. For example, here, I, I specify something like a system. Okay. So the goal for this message here is the role of system and the content is your child who doesn't know anything about baseball. So I basically instruct GPT what kind of role it takes on now. It's not the helpful assistant anymore, but it's a child who doesn't know anything about baseball. And now I specify for the user for me the, the same question. And now it should respond differently. Okay. It worked. It didn't work. I did it, but let's see. Now it tells me I'm sorry, but as a language model, I cannot provide real time information. So it's not what I was expecting because before it answered differently, it told me I really don't have any clue about baseball, you know, like a child would do maybe. And now it answers somehow differently. I don't know really why. You can say, okay, because I provided that it doesn't answer my question anymore. Okay. It tells me okay, no, I don't answer this question. So you can, you know, instruct your GPT how to, how to behave to some extent, you know, and why I didn't get this child thing. I don't know. Let's see if there's something else that people, you are a pirate. Well, that's, it's just your pirate. Let's see what it does now. Yeah, now you can see it answers somewhere differently. R, me, me, me, you be stretching outside, you know, so it's answering the question like like a pirate would do. I felt the room was telling of the Los Angeles coming out of the top willing the works here is blah, blah, blah. So you can basically, you know, tell GPT how to behave and, you know, especially for many use cases within operations and so on and also for teaching, you know, something, you know, you might be interested in because you don't want to provide the answer. You maybe want to guide your students, you know, through something without providing the answer. So you can tell your GPT to behave them in that kind of way. Okay. All right. Any questions so far. So again, it's pretty straightforward. You know, it's all we need. Using Open AI APIs is really piece of cake. Now one more thing I wanted to show you is these function calls. This is basically what you also do with with those with those plugins. Okay, and chat GPT. So the function calls as I scroll down is basically an adjustment. So this is my call to GPT again, you know, you have the model you have your messages. But you also can specify functions. Okay, you can specify in your call to open a functions. And these are basically functions that you can define and Python. That chat GPT can use or not again, this model here, it's a little bit different model. It's the GPT 3.5 turbo 0613. Okay, you need the 0613. So that model is trained from Open AI to make a decision if it uses certain functions or not. The regular model doesn't do it, but this 0613. It can decide if it uses uses the function or not. Now, what you what you what you need to do in order to specify functions is the following. First, you define general Python functions. Okay, so I have two functions here. One is get current weather and another one is get mood course content. So just two functions I want to use. Maybe here I specify I have just dummy, you know, functions that returns always the weather and San Francisco or some location, I don't know, some location, you know, I just specify always the same, the same values here. So the other function get mood course content is just a function where return with my mood token, the content of a certain mood course. Okay, it's basically our our mood from from our from HW and it will give back basically the mood course of any any course you have in your mood. So, but in order to in order to have chat GPT decide which function to use, I need to describe my functions now in a certain way. And mainly I, this is basically how I do it. I have the description of the first function looks like this here. You have a name, you have descriptions and you have paramedics. What's important is the description. Okay, this is what what chat GPT uses in order to determine if it uses the function or not. So here I describe get the.  whether in a given location that's basically what the model GPT 3.5 tour boat the 0.613 will use in order to determine well I think I should use this function now okay and it also describes the parameters what does it actually need it needs a location okay it's of type string and here's a description of that parameter the city and state for example San Francisco it's always good to also use examples okay we'll see that later on often very often also in the paper from Stanford that they use examples okay this makes very easy for chat GPT to understand what what is it about here okay so we describe everything here in chasing format we have descriptions the properties everything this is how basically chat GPT understands what to use and how to use it and the same is what we do for the second function here get model course content description get the content of model course we need the parameters what do actually need for it the course ID well it's the ID of model code model course for example 36301 okay it's of type string and here we say it's required without that we cannot call the function okay now let's let's maybe first do this message now I put in the first message now into my prompt okay I'm the user again content what's the weather like in Boston okay now I execute this message well it's first maybe load all of functions all right now let's run this message now we get a response and let's take a look at the response this is how it looks like so you get basically a response yeah it worked so it tells me message is assistant content is now now so it basically gives nothing back in terms of content but it tells me look there's a function call so it determines correctly that at this point it cannot reply anything because it first needs to to call a function and the function that we need to call is here get current weather so it tells me the function name that we need to call it also tells me what kind of arguments we need okay the arguments are Boston because I asked for Boston so the arguments now the location will be Boston so I need to to call this function with the parameter Boston okay this is what I get back now what I need as a second step now here I need to call the function because this is not what Chech if it either it's running in my code it's running on my machine so the call is done from my back here okay so what I do now is basically I read out the response message and if there is a function call you know within the response message message I need to call this function so I need need to get from Chech if it the information what function do I need to call it's in the message right the name of the function I also need to to load the function arguments so basically in the in the term arguments you know that's what's in here so the function name and the arguments that's what I need to read out here so I have this little function look up I have two functions with the name and now I tell okay basically function look up by the name the name was what was the name get current weather so that's my function name and that's the function to call and then I call this function get current weather with the function arguments Boston okay and get a function response so if I execute this part here I get the function response okay as you can see here now basically a JSON file Boston there's a 2 sets temperature 20 27 forecast sunny windy you know that's my dummy function always gives that back and now I what I now need to do I need to provide this to chat GPT in order to generate the final answer okay so basically I give back my response message I append my function response okay here I limited to to 10,000 tokens because otherwise it cannot execute it it's not very large in this example I would need that but for for the later I need that and and then I call chat GPT with those messages okay so it's basically also the messages from my function response so if I execute this part here it gives back basically what you see here the weather and Boston is currently sunny and windy with the temperature of 72 degrees so now chat GPT creates this function response and makes a nice text out of it okay and now this is what you can do also with with another message so let's say I want to do my moodle okay I want to get the content of my moodle so I run this message now can you get me the content of moodle course 36301 I think that's the the content from this year usw last year I don't know so I will run this one now so it should determine which function to call because it's a moodle course now it should determine I think I need to run this other function let's describe here get the content of moodle course now you can see this is what I get back here so determine correctly if we look at the function response messages response message that was the first one okay so okay but you can see it's basically it took you know the the information from moodle and it generates the response so basically what you find in my moodle class production slides even Google engine so it's not from this class but from the last class all the content that I had in there and you can see now that there's links so I can also use those links in order to to download the slides and so on so I can build basically a nice plugin you know for moodle so that I can use the chat GPT with my phone you know I'll talk to it and say hey can you download the moodle stuff you know that I mean as a professor I can you know also have interesting interesting use cases where can you know let chat GPT summarize your work and you know and and do first you know summary for for all the students and things like this you know you can think of nice various nice use cases but we will talk about the use case later okay now this these are the function codes you know basically you describe functions in Python you describe it in natural language provide the functions descriptions to chat GPT and chat GPT can use those functions and this and can decide on functions and any point in time you can use your functions anyways in your in your in your Python code but here you have the options to use a model that that's more enough to decide what what function makes sense at which point yeah all right now the next thing I wanted to show you is if you really don't want to work with chat GPT you know because of money or there are other you know situations if you have for example the privacy issues you know if you are working for I don't know the shaliteo so you have your dealing with health related data well you cannot just use open AI because you're sending your your private data through you know through through the public service so in this case you need you need you have your own local you know computing environment with your local models and this is possible too and the best library in order to work with local models on your local machines and also with publicly available models is the hugging phase library okay so let's maybe just take a quick look at the hugging phase library so this is look this is how it looks like it has here a model section okay and here it has for all different sort of tasks it has the corresponding public models okay so there are different tasks such as computer vision so image classification image segmentation object detection then there's also audio you know a speech to a text to speech for example or text to audio so if you if you talk to your phone you know to to your app that translates into tech  and so on. The mostly use of course is natural language processing, especially text generation. This is basically what check GPT does. It's nothing else than the task of text generation. Okay, so if you click on a task here, it basically gives you all the public models. Okay, that are available. You won't find GPT3 or GPT4 because OpenAI decided to not make it public anymore. GPT2 is still somewhere here. It's public, but not GPT3 or GPT4. Okay, and you can see there are lots of models. Some famous ones are here, Lama 2. Lama 2 is basically from from beta, Facebook, and it's quite good, you know, and as you can see here, it always tells you, also if you click on it, it often tells you what's the size of the model. Okay, so you can read through certain things, you know, what it's about, how it was trained, what kind of data they used. And for example, here you can see Lama 2 comes in different range of parameter sizes. So Lama 2 here, the smallest one has 7 billion parameters. Okay, 7 billion parameters is pretty much the most that fits in a typical laptop. So Lama 2 with a 7 billion parameters, I can still run on my laptop. It's pretty slow, but it still runs. Okay, 13 billion, 17 billion, 70 billion doesn't run on my machine anymore. Also it doesn't run on my GPU, my GPU is 16 gigabyte, and it doesn't fit on 16 gigabytes GPU. I have about 32 gigabyte RAM. There it fits on, so it's running on a CPU here, and that works. Okay, it's slow, but it works. So for testing, for example, you can you can use it. And there are, you know, pretty, there are some other ones. So for example, Mistra, that's supposed to be even a bit better than Lama 2. It's from it's from a startup in Paris. And now just two weeks ago, they published Safe here. It's basically a fine-tuned version of Mistra. And what fine-tuning means and how you do it, it's all again next semester. Okay, but again, Safe here right now is the best performing public free-to-use language, large language model that's out there. Okay, but again, you know, it's published two weeks ago. So every, you know, every other week or so, you get new insights, new models, so entire world is basically working on improving those models. And this again, this is also seven billion, seven billion is kind of the threat of course, these kind of models still fit on regular laptop. Okay, if you later on would like to work on a larger model, it's also possible we have this Kei Werkstatt in HGW, and we can collaborate with them. So they have a pretty good infrastructure that also runs 70 billion models, for example, without any hassle. So if you want to try out later on larger models, you know, we can discuss about it. For example, this one here, it's a health specific model, 70 billion. If you would like to test it out on some health-related data, we can we can talk to the Kei Werkstatt, get an account for you and you can try it out. But it's a little bit, you have to have a special key and everything so we have to set it up for us. So it's a little bit more effort to make it run. But again, keep in mind we have an option, so if you're interested, just talk to me. Okay, so again, this is the hugging phase and the nice thing is, hugging phase is pretty easy to use too. Okay, so this is basically the library, transformers, it's called transformers because all those models are based on a so-called transformer architecture. This is why it's called transformers. Again, the transformer architecture, I would introduce you to next semester if you're interested in that. Okay, so what we can use here is use the, let me use another. I'm solely here, so we import the pipeline here and now it's as easy as we just use the pipeline and we use certain tasks. So in our case, we use text generation, but again, you know, there are all these different tasks. If you want to do something with computer vision, use some of those tasks, okay, death estimation or image segmentation. Now, there are all these different models that you can just plug in and play and go. Okay, again, also if you want to have some more duplication, so voice activity detection, you know, just use those corresponding models. So in our case, let's take text generation and we use the pipeline for text generation. We initialize this generator here, suck, and per default. Yeah, per default, as you can see, no model was supplied. So if we don't supply a model, it takes just default and here you can see the default is GPT2, okay, because GPT2 from OpenAI was still public. I think it was back in 2021. They use GPT2 and this is basically still public. So you can use it with hugging face. So basically, if you look at this generator here, you have somewhere your model, let's say, model here, GPT2, special version of it. And so what you can do is somewhere here, you can look up all the, so here's the transformer, all the weights, you know, that are in the transformer architecture, basically all the seven, in this case, I think GPT2 had about 160 million parameters. So all those 160 million parameters, you can look up, you know, somewhere all the weights. So basically the weights are the parameters and you can look them all up. Okay, they're all available. So basically, you have a downloaded model of GPT2 here. And again, what those weights are and, you know, what they do, again, next semester. Now, let's just use it, okay. So we asked the same question who on the word series in 2020 that we just asked GPT3. Okay, so let's just run it. And now here we can see the answer. So who on the word series in 2020? Now you get the answer. You, Jackman, we had the very same time, and I think we'll be doing other shows by the time we get to the end. The most important, I don't know, let me put it in. Yeah. Now we get the different answer. We are delighted to be making come back for our latest title, The Great Bitter Spake Off in 2008. The series won the best bake off award alongside the Lord middle that mid. Okay, so that's the answer from GPT2. Okay, now you can see typically, you get a lot of crap. Let's try again, you know, at least there was something about a series, you know, that's already better than a, than I saw a lot of time before. That's the term. That's the question. It has come. I think outside the world, a good question for a lot of us here, I think that we've got to pick a choose because of that. So, complete crap. Okay, you cannot really use it at all. It's just random tokens, more or less, you know, that are grammatically at least, you know, it's not just random tokens, but there is some grammatics around it. Okay, and this is what GPT2 generates. I had one project in this class where Stuart used GPT2 because it was new and this student generated a rap text, you know, because of rap, it didn't need to make a lot of sense. And it just, you know, used some keywords with money and then sort of typical rap text. So in this case, it kind of worked. He find tuned it to rap text, English rap and then it was somehow made sense. But this is kind of how limited it was, you know, for certain scenario, like rap, it was kind of okay, you know, but it was far away from being in any way intelligent or smart or, you know, in any way, usable. And this is again, you know, it must be, and the difference to GPT2 and GPT3, it's the same architecture. It's basically a transformer architecture. You will know this architecture next semester, but the same architecture, you know, was just more ways. They just basically added, you know, GPT2, 167, GPT3, I think had 70 billions. No, GPT3 wasn't. I think GPT3 was the best way to do it.  had 160 billion then, you know, from 160 million to 160 billion parameters, they blew it up, you know, that's it. And they trained it again. But, you know, they didn't have any clue how good it would perform. So the first time, you know, the training was finished and they asked the first problem, the first question, you know, even open AI must have blown away by the quality of the model. And I don't believe they knew what, you know, what they just developed there before they, before they started. So that's pretty, you know, is a pretty large milestone that, you know, it's definitely unexpected. And yeah, just to illustrate. So, but again, coming back to to hugging phase so we can also always save those models to our machine. So I have all these models here on my machine. And you can load it again from your machine. But I think once you use it, once it's anyway, somewhere in the cache, so it's you don't, you don't even need to actually store it. But, you know, you can put it in your repository and then put it somewhere else and still use it. So let's, let's try one more different model. So this one is blooms. I'm not even sure where it's from, but it's basically big science blooms. You can also find it here on on hugging phase. So let's just take a look, you know, big science blooms. So you can open it up here on hugging phase. And again, you get some information. I'm not even sure what's behind blooms, Nicholas, meaning half, I don't know, he's within the corporation or not. Maybe this was private. But again, this one is already a little bit bigger than the GPT-2. So we can use the blooms. So we just load the generator blooms and ask now the blooms generator. That's not loaded yet. Now, let's use blooms. And now you can see here one of the World Series in 2020. It says New York Yankees. So you can see at least now it has something to do with baseball. The answer I think is not correct. I think when we asked GPT it was dodgers. So Yankees is not correct. It doesn't give the correct answer, but at least it has something to do with baseball. So it's a little bit better with 1 billion parameters, but still not really helpful in many ways. Now the next one here that I'm going to use is Lama 2. Lama 2 with a 77 billion. That's a small Lama 2 model from Facebook. And this is already that large that it kind of slows my machine. So I ran it before, as you can see in the console, I did it before I started the class. So it took about 1.5 minutes to load the model. And I asked the same questions to Lama 2. And you can see here, and also the answer takes about half a minute. But you can see now it answers the Los Angeles Dodgers one to 2020 World Series defeating the temper by racing six games. So that's a correct answer. That's a correct answer. It was six games. It was against temporary base. And it generates a full sentence, not just extracts the answer tokens. It generates a full text. And so we can work with that model. And it's for free. It works on our machine, but it's somewhat slowly. And again, like GPT-3 on GPT-4 sometimes does hallucination. hallucination is basically the process of generating something wrong. And you don't know. Also Lama 2 hallucinates. So I had one I think the last time I ran it, it actually gave back the temper pay race, one the series. Okay, so it was the wrong answer. It confused dodgers and race due to some reason. Okay, so it's still not bulletproof. It's better, but again, hallucination is an issue. And we have to be aware of that, you know. If you use the Lama 70 bill, and it's probably better too, you know, but again, that's a little bigger. Also, I want to work on with those models, you know, I can provide you virtual machine on HW, just in email, you want to have a virtual machine and those models should work on the virtual machine too, because it's typically 32 gigabyte of RAM. So it doesn't have a GPU, but you have you should be able to run it on a CPU. Okay, and what I would do is again, first test it maybe on GPT-3 if the prop makes sense and then run it here on Lama 2 because it will take some more time. No, it's slower. Okay, any questions around that? Part? Again, face. Okay, so then I think we should do like a 15 minute break. And after the break, I will talk how you basically link private data so far. We just ask questions that are public, you know, but what if you want to work with the data that's private? Okay, and I will show you how to do that with with generative AI. And then I will talk about so-called generative agents. Okay, so agent systems that use lots of prompts, you know, and having made basically prompt-based conversations in order to accomplish certain tasks. Okay, so we'll do this. Let's continue at 9.45. Right? Okay, everybody. Will you go Everybody? Yeah, I'm going to screen again. So, okay. So, what I wanted to, so far we only talked about basically using large language models that are trained on some data and provide certain answers. But the question is really what do we do if we, if we, if we have private data and want to have information about this private data. So, and this chart you can see basically the upper case, you know, where we have GPD. And then we have some, some questions that can be answered by GPD because it was trained on data that, that, and basically the information that we're looking for. And what we're now talking about is the case below here is basically you have some private data. So, for example, you can imagine you upload all your emails, you know, I did it once. I basically upload my entire email database, you know, to a so called vector database. Okay, and this is what, you know, is used these days with these large language models are mainly used with vector databases. So, there's a lot of novel kind of database providers out there providing these vector databases. And what happens is that, for example, if you provide any document such as an email, your email gets first embedded. Okay, and these embeddings are basically vectors of numbers that represent your document. Okay, how these embeddings are generated. Again, this is what we talk about next semester. Okay, these embeddings are nothing else than weights that are trained based on a large corpus and open AI did it. They basically trained their models on the entire internet plus in our books that were written and so on. So, basically the entire corpus of our world is used not really the entire, but you know, a large corpus is used in order to train GPT models. And also these other large language models are trained on very large text corpus. And doing this basically allows us to encode every token with a set of numbers that represent basically the meaning of this token. Okay, and the same is true for documents. There can be transformed into vectors in a vector of numbers, as you can see here, that represent basically the meaning and the semantics of that document. Okay. And if we work with private data, the first step is to take your private documents, be it emails, be it your WhatsApp messages, be it your CV, be it your medical records, and you need to represent them.  with these vectors, with these embeddings. And I'll show you later how to do this, programmatically, not how to train models in order to generate rates by taking a private document and generating a bit that represents the meeting. The document. Is there any question? Now, so then we store those embeddings in a vector database. This is basically the representation of our documents. And once we have a query, a prompt, for example, we want to ask some questions about our emails. This question is also transformed into an embedding. It's also transformed into a vector, as we can see here. So in this example, it's what I did is basically, I took all the emails from Anthony for C, who is Anthony for C. He's basically the villa from the US. So basically in the COVID crisis, in Germany, we always had this guy, a villa from the RKI, who was announcing what we need to do in order to be saved during the COVID crisis. And in the US, it was Anthony for C. He was the director of the National Health Institute and he was in charge of the infectious diseases. And so he was the one. And I think the entire US thought that he's doing a really good job, basically bringing the US safe through the pandemic. At the same time, we had Donald Trump, and as you can see on this picture, Donald Trump had his thesis that you just take some pills and then you'll find, and you see how Anthony for C reacts to that. So it was kind of a funny team, Trump and for C managing this COVID crisis. Anyways, Anthony for C's email, it's made public afterwards. So his entire email account is public. So what I did is basically not mine, but his email account, I put into a vector database. And now if you want to ask a question, for example, how did Anthony for C community communicate privately about the COVID crisis? This question is transferred into embedding. And now this embedding is compared with all the documents, with all the emails that are also transferred into embeddings, also in vectors. And based on their similarity, they use the course in similarity. So they basically calculate the similarity per vector. You choose the most relevant, the most similar vectors to this question. Put those documents into your front as a context and then answer the question. Okay, so the prompt to ChechyBT in this case would be, how did Anthony for C community privately about the COVID crisis? Consider the following context. And then you provide all the private emails that are relevant to this question. Okay, of course you cannot put the entire email account from Anthony for C into your prompt. You only have a limit of 4,000 prompts, or maybe if you take the large model, you have a limit of 16,000 prompts. But still, that doesn't cover the entire email communications from Anthony for C. So this is why you do the similarity search. Take the similar parts that are similar to the question out of your vector database, put it into your prompt, and then you answer, you ask the questions to GPD. And GPD will answer this question based on the context. That you provide in. Okay. So this is how it works if you work with private data. And this is how all the companies do it now. If they have their private chat parts, you know, focusing only on their documents, focusing on their content, well, this is what they do. They take all their content and bet it, put it in a vector database, you know, get a query, take calculate similarities, take chunks output in the prompt, put the prompt to GPD, get an answer. Okay. That's how everybody does it with private data. Now, how do you do it in Python? This is what we're gonna cover now. So I have this script, your pine cone demo. And by the way, I put all those scripts here on, on Moody, okay. So you will find them afterwards on Moody. Right now you don't need to open them up, but don't worry, you know, you will have them on Moody. So, for this script, I use one vector database. It's called pine cone. It's one of the most popular vector databases, I would say at this point. So pine cone is one that I use. It's, it's, so the data from Anthony for C is on pine cone servers. Okay, it's not local, it's on pine cone servers. You can also use a local version. A local, a local version of a vector DB, a database is ChromaDB. ChromaDB, you find the link here, runs basically locally. So you can put all your private data here on local, on your local machine, and then do the same what I do just with a local database. So if it's really private health, health records. So you don't even wanna have it on a, maybe on a cloud-based database. So then I would use ChromaDB. But here I use a pine cone. So if we go to pine cone, at this you can log in. And also you can sign up for free. You know, you get one instance here, one index basically they call it, you get for free. So here's my index called for C. And if you take a look at the data, some samples, this is how it looks like. So this is for example my first document. Yeah. This is one email. Okay, and you can see basically the entire content of the email is stored in this metadata here. So the body of the email. So basically that's the text. The attorney of OSFAD, we're short, when the good doctor forces speaks blah blah. You see the CC, you see the email ID, recipients, so the recipient was in for himself, the sender and so on, the subject and so on, the timestamp. And what you also see here are the values. And the values, that's basically the embedding. That's the vector that represents this email. Okay. And you can see basically the dimensionality that we use for those vectors is here. It's 1,536 again. All this will make sense next semester when we understand what that exactly is. And but here just remember every email, every document, every chunk of text is represented by 1,536 numbers. Okay, these are the numbers here, some weights in the end. Okay. And we use cost and similarity for similarity. So if we have a query like here, we have a query that's also just the vector, okay, that's again, just 1,536 numbers. We just calculate the cost and similarity between our documents and the ones that are here in this case, with this query, I take the top 50. So I take the 50 documents that are closest to this vector out there. Okay, so here's the second email and so on. Okay, so this is how vector database works. Okay, just metadata and embeddings. That's it. And there's also ways, by the way, to query against metadata, you can query against time, only filter the ones that are a certain time, that's a regular, how you do regular queries against the database, you know, you just take the data, the metadata here. But the new thing here is that you have embeddings and you can also query based on embeddings. So this is what we do. So we import here our fine code, let me take another, let me close this one now. Let's take them one. Now, so you also need a fine code key, I have it in my configuration file. Okay, now here in this line, I connect to my index. So I connect to a database. It's called for C. And now I want to ask a query. Okay, someone told me something about Zuckerberg being, you know, a relevant for Anthony for C during COVID crisis. Okay, so what I want to do is to get this data database. So I want to get this data database. So I want to get this data database. Anthony for C during COVID crisis. Okay, so what I want to do is basically, what did Zuckerberg offer to Anthony for C during the COVID crisis? So that would be my query. And let's see, again, it's not public. So it's private and it's in his private emails. So this is what I do here. I need to do what for this query, as you can see in this chart, first, I need to embed this query. Okay, I need to generate a vector  represents this query, the semantics of this query. Okay, and this is what I do in this part here. I use basically OpenAI because OpenAI has very good vector representations. Okay, a few of this is also documented here. Here basically explains their vector embedding. So any text that you can take can use the OpenAI embedding model is quite frankly OpenAI embedding model is the best. So it's the best model that generates from a text vector of numbers. That is a very good representation of that text. Okay. I think he's also charred basically vector is, you know, you can. So similar text are basically similar vector space. So here's a three dimensional space, you know, but in our case again, OpenAI uses 1,536 dimensions. So it's not a three dimensional space, but the space in fact is 1,536. You cannot illustrate it. Of course, you can only illustrate a three dimensional space, but you can see a similar document. I an a similar space. Okay, so here are the documents around animals. Seems like we're here about village, here about an athlete. The blue is about a film and and purple is about transportation. Okay. So it's basically a clustering of documents. And these are represented in those in those vectors. So what I use again is OpenAI embedding model. The model that I use is this one here text embedding ADA 002. That's right now the standard OpenAI embedding model. And I also played around with different ones. This one really works the best. Okay, the best representation. So we just use it. And again, here's our query. Take it as an input and create the embedding. We generate a lot of data. So we adjacent file and we navigate to the embedding. And we have a embedding here as you can see here is a list of 1,536 numbers. Okay, this is our dimensionality. This is these are all the numbers. These numbers represent now the question what did the book offer to enter the foresee blah blah blah. Okay. And again, make sure you have your limits because at this point, I already used again the OpenAI. So for using embeddings also cost. Okay, not much that's even less than using a prompt, but still there's there's cost and more. There are also embeddings that are for free. For example, cool here. These are completely for free. But again, they are not as good as the OpenAI wants. And the next thing now is I use this embedding. I just use the vector of numbers, nothing else. I don't use the text or the timestamp or anything, just the vector of numbers. And ask it my my kind code database. To give back the 10 most similar documents in the in the database. Okay, this is what I do here. So I get 10 mattress. And basically, you know, emails that are somehow related to to the question. You can see the first one, for example, for email below Mark Zuckerberg has extended a few office to do videos with you. So it's basically email or someone writes to Anthony, you know, should we work together with Zuckerberg about this and not. And this is basically the context that you need in order to answer this question. Okay. So I don't have a set of emails, but these 10 emails have some relation to our question based on course and similarity. So now I'll take the context. You don't need to just take the body, the mid the sender recipients. And I think the body. From those emails, the rest I don't take. I glue them together here in a query. I also use a primer here. You don't need need that really in order to, you know, specify. And should do. And here we again have our call to our GPT here. I use the GPT form order. This is the primer, the system message to how, you know, GPT should behave and my augmented query. It's basically my query that's augmented by the 10 emails that are in my private database that I. Made now not private anymore because I use GPT, but I also could use it with my hug and face private model. And then it would still be private. So let's, let's run this here. Now we have the result. So now basically GPT. Summarize those 10 emails. So it's basically Mark Zuckerberg's offered the opportunities to do videos with him. These videos were expected to carry significant wave. So that's basically jet jet GPT now taking the context of those 10 emails and providing the answer based on those 10 emails. So this is, this is basically how you can develop the take the data and work the same way as jet GPT, but also has access to private data. And again, once you use GPT models, of course, it's not private anymore because it goes to, you know, I use the API. So I send it to the open service. But again, you can also use the number two or the private models for that. If you want to. But also mine, I mean, this is data. So even if you if you don't worry about privacy at all, but just have private data that GPT has no access to that would also be the way to work with it. So I'm just seeing here as a question and a chat. So the recordings did I continue the recording my way? The recordings I will put on wood. Yeah, I will do that to make sure just that I continue the recording. So yeah, continue. Yeah, so the recordings will be on a model. All right. So any questions regarding that? So it's all as you can see. It's not just using API. It's playing code interface open AI interface. If you use chroma, you look at the chroma description. It's all two three lines of code typically that you need. Of course, you need to make sure to embed your stuff before running it, but it's all also in the descriptions. And so it's already in terms of coding. It's not it's not too much, you know, effort and it's not too too challenging. I have a question. And I wanted to ask. Because I was thinking about something like making a project where you can. And so, the thing is, the thing is, the thing is, the thing is, the thing says, is going to be implemented in the next request. Like, is it possible to. Put the output of the activity in the next input? Yes, yes. That's what we did here, remember. With the, with the Dodgers, you know, so on the word serious, we got a query. And we got a response. And now we had to put the query and response into the messages. That's what we did here. And you would see later on, you know, that's, that's leading me towards my, my projects that are. That are all something like this, you know, I use the change of the output in order to programmatically do something else and, you know, build complex systems around, you know, handling prompt. And then, you know, a little bit patient, I will get to that in few minutes. Okay. So, private data, vector databases, retrieval. You know, you have retrieval here. We use cost and similarity to retrieve documents from the database, put it in context and work with it. Another thing I wanted to just quickly mention is, Lang chain, Lang chain is a, it's kind of a framework from, from, that's, that's out there that many people use. Some people say it's not very valuable. Some say, some say this. I just wanted to make you aware that it's out there. Okay. And maybe just quickly. Documentation here. So this is Lang chain here, the web page. And again, it's, it's basically a collection of, of a lot of interfaces that make your life when you work with more complex systems. And there are driven by language language models, make it a little bit easier. Okay. You have so different interfaces and modules, for example, retrieval modules that, that allow you to easily use pine cone cohesion.  year, chroma DB. So they all have interfaces for those that make it a little bit easier to work with those. They have chains, so we have chains of models that you can run. And they also have agents. Agents are really defined as also large language models that also have a set of tools available. That's somewhat similar to our functions in open AI. And this is what I just wanted to illustrate here. So again, let me import my. Open the new. So again, my configs. So, and here we have our, so you can use Lenshane here. This is our Lenshane. You can use it for example with hugging phase. Okay, so it has an interface to hugging phase to the hugging phase pipeline. So you can here, for example, use the hugging phase pipeline. You can also use the, the, the, the, the Lang chain. Hiding hugging phase pipeline. Okay, it doesn't make a big difference for now. But just remember, you can just use this one here. And use hugging phase pipeline from Lang chain and initialize the same thing as we did with the original hugging phase pipeline. Okay, and then we can ask a model here. What's serious again or blue. Well, as long as we wait for the answer, the same is true for open AI. Okay, so from Lang chain, you can also import open AI. You have to specify the key for open AI to. So you can use open AI through Lang chain. Okay, so this one here. Skip this part. So here we use open AI. Okay, now you see the answer again. Well, Andrew's daughter is wondering what serious. Okay, but the important thing now is the agents. That's the interesting part. Okay, and here we implement from from Lang chain a certain type of agent. So we import first year couple of things, but you can also see some utilities. For example, here we import the Google search API of rapper. So for example, here that's we will later on define tools. For example, a search tool and Lang chain provides already already a Google search tool for us. Okay, we don't need to tap into Google search and read how the API works. We can just use the Lang chain, Lang chain tool for that. Also, Lang chain provides a math chain here, for example, it's also another tool. We can define. Okay, and this is what we do here. When we initialize our agent here from Lang chain. We hand him this agent him or her set of tools. Okay, we we had her let's say Our language model in this case it will be open AI language model and we initialize a certain type of agent here. It's zero shot reactor description agent and we didn't initialize that the tools that we give our agent to her hand are two one is search and one is calculator. The search tool is basically this one here search that's the Google Google search API rapper from Lang chain on the math is basically the math chain from from Lang chain. So every person can also add basically change to Lang chain. Okay, so if you have a chain you can add it to Lang chain. Okay, basically chain of prompts, for example. So we provide those tools to our agent and now we do the same question as we did before with Leo Leo de Capri. Okay, and you will see now why I actually did that. So if I see If I run this entire thing again, the code is not so important right now it's important to understand what it does. Okay. So I run this this basically this this agent now and it has two tools to tools at a hand and a question and a language model. Let's see what it does out of it. Now you can see here it's it's doing a couple of steps. Okay, it's basically, for example, here it enters a new agent executor chain I should search for Leo Leo's girlfriend. So it always is prompted in the way that it should, you know, write down what it does and what it thinks. So here things I should search for Leo's girlfriend and then calculate to solve the math math questions. So the first action it does is the first thing that I'm going to do is I'm going to do this. It takes a search action. Okay, action input is Leo de Capriose girlfriend. So the observation is okay. It goes on the web it loses uses the first tool the search tool which is Google Google search and get get some information. I now have enough information to answer the original question. Final answer Leonardo de Capriose girlfriend is at her current age to raise of power is approximately 77 years. Okay, that is not correct. And I don't know why. Now you can see I did the same before and I got the correct answer. Let's try it again. We get it right. I don't need to do this. Let's initialize again. It will get better. Now we can see a different chain again, hallucination, it's every time a different thing. And again, I will talk later about how to make sure that you don't run into things like this. You know, you have to. This is where the problem engineering comes into the game that you make sure that something like this doesn't happen. You know that you that you have a somewhat reliable system, even though, you know, the outputs can be different. You know, but you have to make certain you know, and your prompts to make sure that it's not different and there's certain ways how to deal with those systems in order to to make it less probably that you that you have different outputs. But now you can see the correct output. Okay, so again, it searches finds Leonardo de Capriose girlfriend. Now it searches again for the age, you know, Victoria Seretti age. It comes back. Okay, Victoria Seretti is 25 years old. Now it takes a different action takes the calculator action because it has calculated tool. It can take the calculator action. And now it uses the calculator in order to calculate 25 to the power 0.43. It goes into the math chain and do that's the calculation and the correct answers zero is the 3.99. Okay, and that's correct. And Python here 0.43. You get to 3.99. Okay. And you could see when we asked chat gbt about that question, where was it? We didn't get the correct answer. Okay, chat gbt gave us back. Okay, 3.17. So again, gbt for the, you know, the best model of the best, you know, version cannot calculate, you know, this calculation. So this is what we need to be aware of. And using those tools and certain prompts, well, we can get the correct result. But we need different, you know, tools. And for example, the same way we'll go in an open AI. I have this like this. I think there is a plugin from from all from alpha alpha. Once it gets it understands I need to calculate it gets into all from alpha and does a correct calculation. Okay, but you need those interfaces. Okay. So also, how does that shape does it? So let me just see if I find the agent description. You can see basically all what's going on with the agent. It's zero shot agent. Let me see if I find the end chain. Here's the prompt basically that it uses. And here's the prompt template. So that's basically what it does is, it uses a prompt template within its code. Within its code. I always have this window. And this is how the prompt templates within length chain agent looks like. Okay, it basically specifies answer the following questions as best as you can. You have access to the following tools. And then the tools tools I gave the agent it has search and it has a calculator use the following format question the input question you must answer thought you should always think about what to do action action to take should be done one of one of one of your comes the variable search and calculator action input input to the action. So the input to that, you know, the action selected then observation what's the outcome. He had says this thought action input observation can repeat and times. So this prompt health GPT.  you know, you can do it several times. Don't stop necessarily after the first time. You can do it several thought I know the final answer, final answer, and now begin question thought. These are the variables. Okay, so that's basically a pro template that's filled for all these questions that you that you ask whenever you run an agent, you know, that's filled with those variables and the agents executes those those those from templates. Okay, and this is quite powerful and this is what what would you see more and more also research papers, you know, popping out on a daily basis where people find out how do I actually work with these systems, you know, again, GPT, those large language models are black boxes even for open AI. It's a black box. They can find you and they can adjust it to some extent and we talk about this next semester. But in the end, it's a black box even open AI doesn't know what this AI is actually doing and you know, and so people are researching now in the area of how, you know, can I make this novel AI and that's out there now, that's also changing constantly of course. How can I use it in a way that's predictable? And this is again, it's a new field of research, but it's it's it's now a field that that everybody's kind of focusing on all the I work with this with AI group and Stanford myself, and now they're all suddenly working with, you know, on prompt engineering. And this is something I wanted to to show you now next. So again, there are these different from frameworks that that go about on a constant basis. And they are basically just guidelines, you know, simple ones, for example, like this, okay, you have a role task format, that's the RTF format, okay, always specify a role, you know, and then and also a format, you know, it helps GPT and all to do what you would to understand your situation and what you're looking for, you know, or simple things like let's think step by step, okay, improve what you see and to get better results. I think for that I have an example here, down here, I asked it, I think yeah, up to over 18, I basically asked GPT4 aside from the Apple remote, what other device can control the program, Apple remote was originally designed to interact, it was from some paper whether it is the test. And for the ones who know Apple remote is basically an old tool that was used in order to control something called front row, it was back, I don't know, 2005 or so. Apple had front roads in old old system, but you had Apple remote in order to control front row. So the original device was basically, or the original program was actually front row, nothing happens, okay. And if you ask GPT this question, it doesn't get it, okay, it doesn't get it still, you know, GPT4, newest model, just language, it's not calculations, it's just the question, it doesn't get it, it will answer something that's not has nothing to do with front row, okay. But the same front now, I asked the same question, but at here in the end, let's think step by step. And now what happens is it completely answers the question correctly, it takes front row entertainment and explains what it was and how to control it and answers the question correctly, okay. So just by adding, let's think step by step, you kind of instruct this AI, you know, to think in a different way and it doesn't, okay, and gets better reason. So these are kind of things that people are starting to find out and you know, giving guidance what to do, you know, this rise in framework, you know, that you should specify something or hear rowed framework, for example, examples, you know, that's very important that you provide examples, because then it understands immediately, it learns from those examples, it's just one or two and it knows exactly what to do, examples are very powerful. And if you leave those out, you just have to get a completely different answer, okay. Chain of density is something else. Again, these are not really completely validated, you know, but this is what you see now on a regular basis when you when you, you know, research in this space, you see papers around these kinds of concepts. Okay, now what I want, yeah, that's a front row example. Now what I wanted to maybe talk about, now is one paper I mentioned that before that came out out of Stanford, that was, you know, that kind of went viral in this in this, uh, generative AI community on Twitter and everywhere. It was about generative agents, okay. And what those Stanford people basically did is they came up with a with a simulation of basically people, they wanted to use language, large language models to simulate people, okay. And they implemented this in this nice 2D environment, okay. So basically just to to to have a better understanding what people do and because we don't have real live and feel it's a sensor that they didn't want to deal with all the robotics issues, they just use this nice 2D cute word they call it small bill, okay. And how they basically are now simulated humans and really in a way how we behave. I want to walk you through in the next couple of slides. Okay, and I want to start basically with with this with with a front end, basically with a sandbox that they implemented. Large on like this. So basically the first thing is, um, yeah. So they they um, uh, they defined basically small bill as a community of 25 unique agents, okay. And each agent should basically represent a person. And in the beginning, they just provided a basic description of those persons. So basically initial, you know, initialize their lives, okay. And then each person was represented by a little avatar and this 2D word. And for each avatar, they they define basically the occupation, the relationship to other agents in this world. And then some seat memories. So for example, here they initialize John Lynn. So John is a pharmacy shopkeeper at the Willow market. And he laughs to help people. So director is now they specified he's always looking for ways to make process, get him medication. He's living with his wife, Malin, and she's a college professor, his son Eddie Lynn, who's studying music theory. And then his neighbors here, couple next doors, Sam Moore and Jennifer Moore. Um, and and you know, he thinks about them, they're a nice man and his neighbors and so, okay. So basically some seat values and information that that defines John Lynn, you know, in his behavior. Because we'd see later on, John Lynn is completely controlled by CHEDGPT, okay. In terms of what he does and what he thinks and, you know, what he reflects and plans and do sense on. And what would see that in a second. Um, now, the sandbox environment, they implemented it basically in a way. They use the JSON data structure that basically contains for each agent in the sandbox, the current location. So John Lynn's location, for example, a description, or John Lynn currently does. And the sandbox object that John Lynn is interacting with. So with, for example, if John Lynn is going to the coffee machine that's in some room and is brewing a coffee, then this is stored in a JSON data structure that John Lynn is at the, you know, a certain room in front of the coffee machine and is interacting with with the object coffee machine. Okay. Now at each sandbox time step, the sandbox and the time steps down here. So every time step represents 10 seconds. So all the humans in this world do is based on 10 seconds. So I'm always planning ahead for 10 seconds. What I'll do next. I always do daily plans to I would see that later, but it's computed, you know, in 10 seconds intervals. Now at each sandbox time steps, the sandbox server. So that's the basically the front end server process the JSON for any changes coming from the agents, move the agents to their new positions and updates the status of any sandbox objects. Okay. So an example is if John Lynn goes to the to the coffee machine and makes a coffee, we have in our JSON file, we have to set the object coffee machine from idle because for it was not used to brewing coffee. Okay. So that if other agents go into this room, they understand, okay, this coffee machine is not idle anymore. I cannot use it because someone, you know, the status is brewing coffee, so I cannot use it. Okay. Now the sandbox server sending all agents and objects that are within a preset preset visual range for each agent to the agents memory. So basically every agent,  has a certain visual range. So this 2D environment is basically a tile map. And you have a certain visual range, maybe four or five tiles, that you can see the head. That's your visual range. You cannot see much further than four tiles. And of course, if there are kind of walls, you cannot look through a wall. So the front and serve always tells the back and the agent, what kind of tiles, what's the visible area of an agent? Now, still the front end. That's not the real important part, but just to understand what's going on. The front end is implemented here with this tile map and we have areas. So it's a hierarchy-based, you have on top, you have the entire world, small build. Then you have areas, for example, houses, supplies store, cafe, bar, grocery, a college. These are basically the next layer. Then for each area, like for example, the house, you have different rooms here. You have a bathroom, you have kitchen, you have bedrooms, you have garden. And then in each room, you have certain objects. For example, a bookshelf, a table, a coffee machine. These are basically the leaves in your tree. And now later, when we basically use GPT in order to tell us, to do what in our world, we parse this tree into natural language, as you can see here and this in this here, within is in relationship. So if we have a tree that we know stuff is a chive of kitchen, so the stuff is in the kitchen. We render this into, there is a stuff in the kitchen in terms of natural language. And now each agent doesn't have holistic information about small build. Each agent knows only about the parts of the world that he or she has seen. So every agent basically has a subgraph of the entire tree. There's an entire tree of the world, and it's more of a build, but each agent just has a subgraph. So maybe John Lim only knows about the rooms in his house, but he doesn't know about the rooms of his neighbor's house, maybe. So this part of the tree is not in John Lim's subgraph. So every agent has its own subgraph. And again, what they try to do is always represent us in our life. So I know my house, but I don't know the house maybe next door, I don't know the rooms, so in my planning later on, I cannot use information about what's in their house. So this is how they implemented that. So they initialize each agent with an environment tree capturing the spaces. So it's again, initialization, what do they know in the beginning, and now they go off. And again, if you go to a new area, if I go to a room, well, I see what's going on there. I see the coffee machine and then my subgraph gets updated. So my subgraph contains the information that there's a coffee machine in the zoo. And also, when the coffee machine is not idle and I leave the room, the information for me is that the coffee machine is not idle. Because even though the coffee machine is maybe idle then, later in the time, I didn't see that it's idle. Okay, so for that, I agent in his subgraph, this coffee machine is still not idle. Okay, when I go back into this room and see that the coffee machine is idle, then also for my subgraph as an agent, the coffee machine is idle. This is basically the perception step that we also talk about later. Okay, again, if there are any questions, you're not free free to ask, I know it's a little, unusual, this kind of stuff, but you will see later on how it comes together. Okay, so again, the front location movement to determine the appropriate location for each action, which reverse agent store environment, tree and flattened portion to language model. So imagine now, CHGPT later on, we will see planning. CHGPT tells us, Eddie Lynn, you know, worked so hard, he should now take a break and walk around his workspace. You know, that's basically, could be later on an output that we receive from CHGPT. Take a short walk around your workspace. Okay, now what do we do now with the front end? Okay, we first have to let CHGPT know what the options are. Okay, so we tell CHGPT, well, this is a prompt to CHGPT, Eddie Lynn is currently in the Lynn family's house, Eddie Lynn's bedroom desk. So we have to tell CHGPT the tree, the sub-graph of the agent, we're Eddie Lynn is, he's in his house and he's in the bedroom, okay? And this house also has May and John Lynn's bedroom, Eddie Lynn's bedroom, common room kitchen, bathroom and garden, Eddie Lynn knows the following areas, the Lynn's family, house, Johnson Park, how else. So we tell CHGPT now all the things that Eddie Lynn knows, basically we tell in natural language, the sub-graph of Eddie Lynn, okay? And now prefer to stay in the current area if the activity can be done there, Eddie Lynn is planning to take a short walk around his workspace, which area should Eddie Lynn go, okay? And this is a question to GBT, CHGPT. So first CHGPT told us to take a short round his workspace and now we ask him, okay, look, this is what Eddie Lynn knows, where should he go actually, okay? And then CHGPT knowing this information, this context based on retrieval from our tree, CHGPT splits out, okay, the Lynn's family house. So Eddie Lynn should go into Lynn's family's house, okay? And now we do this process or Stanford did it recursively, okay? Whenever, okay, now we know he's in Lynn's family's house, but where exactly should he go, okay? And so it specifies more and more that the output in the end is the Lynn's family house, there it should go and he should go in the garden, and there he should go in the house garden, whatever the house garden is, maybe it's more part in the garden, okay? So this is basically how we use prompts with CHGPT to let CHGPT decide what Eddie Lynn should do, okay? And then for the front end, they just use path algorithms, game path algorithms to animate basically how the little person moves to that space, you know, with a certain speed and based on 10 seconds to the time step, okay? And now the objects, how do we update objects? Status in this environment, again, when an agent executes an object, that's an example we have before making this presser for a customer, for example. So CHGPT tells us later now, Isabella should make a presser for a custom. Well, now we need to change the status of the coffee machine from either to brewing, but we don't want to hard code the possible status of objects, right? We would have to take every object in our world and tell you know, what kind of possible states are for this object. So also here we use CHGPT in order to do that for us, and this is how the prompt looks like, okay? So this is a prompt template that the Stanford people use to quickly, this is how they look like, basically with variables. That you can fill in, here's a comment mark marker, that this is what they cut off later, you know, this is basically prompt instance here that you see, and this is a prompt template. So if you take this prompt template and you generate an instance for it, this is what you get back, okay? So here, if we look at the prompt instance, we want to understand the state of an object that is being used by someone, okay? That's generic description as well. Let's think step by step. So also Stanford use this step by step to make sure it makes a correct answer and doesn't, you know, confuse things, you know, do it step by step. We want to know about coffee machines state, okay? And here you see the prompt templates. It's not just for the coffee machine, this template is used for any object in small village world, okay? So here in the talk from template, you see we want to know about input state, okay? Now input one, in this case Isabella, is abusing the input two. Input two is here making us press a four custom, okay? That's the persona action event description. Step two, describe the input and describe the coffee machine state. Coffee machine is, and now this is what GPT should fill, okay? And GPT understands the context and understands, okay, given this information, I said I give back brewing, okay, brewing coffee, that's the status. So it's not us to determine what the status of an object is, we use artificial intelligence, okay?  in order to determine what the status, the current status for each object in our small will word actually is. Okay, and we use those prompt templates. Okay, and now this is basically the front end. This is basically how they implemented the small will world, you know, and now let's focus here on the agents. This is basically the agents architecture that they came up with that again. This agent architecture is not an architecture that's specific now for this small will world. What they wanted to achieve is really suggest a very general architecture for agents in general. Okay, and this is what they came up with. You have basically certain modules and you will find if you look at the GitHub repository from this paper, you find those cognitive modules also implemented. Okay, you have a perceived module. Perceive is basically what do I see? Okay, what are my sensors basically receiving? Okay, in this world again, it's just a visible area of tiles, you know, that I see, but of course, you can think of us or robots, you know, they have eyes and ears and then, you know, touch sensors in order to perceive much more. Okay. And now what they do, every, every perception is stored into this memory stream. The memory stream is really at the core. It's basically our brain is our memory. Okay, and this is also how they implemented it. Everything that's going on is stored in the memory stream and we'll show every, every module I will talk about in a second. So the memory stream stores all that. Now if we have, for example, here in the back, if we want to do an action, a certain action, you know, let's say, what a brew coffee. Okay. I have to get all the relevant information for me that's relevant in order to brew the coffee. I have to know how to brew coffee. I have to know if there's coffee machine there. And so, okay, so I have to retrieve from my memory all the relevant information. And this is how we use a retrieval mechanism. Okay. Retrieval we saw already before when we retrieved Anthony for C's e-mails. Right. That was retrieval too. And here we have a very similar retrieval, a little bit more advanced as it was in a second. Now we get those retrieved memories and do actions. Okay. But what we also do is planning and reflection. Planning is basically, I do a plan. What am I actually supposed in this paper, they implement first a daily plan. So in the morning, every agent is doing a plan over the entire day. And also basically things that are relevant are what am I, John Lin is working in a store or so. Or there's a researcher somewhere. And when the researcher is researching, you know, he should plan maybe go to the library today, read some papers and then go back and eat some dinner. So on a daily basis. And you would see later on that they break down the daily plan into an hourly plan. And then the hourly plan into five minute plans. Okay. So we have the action then, for example, brewing a coffee come into the play. They also implement a reflection. Reflection is basically that they store things that are important in the memory stream. Okay. For later for later usage. And we would see them also in a second. Okay. So these are the main components. And now let's first talk about memory and retrieval. You see basically how they implement the memory stream. Okay. So the most basic element is observation. You know, basically from the perception when I go into a room and, you know, I see things. This is all stored in the memory stream. So you can see here this little memory stream example way, for example, see in a 10 seconds interval, you know, for example, the last observation in the memory stream was desk is idle. And bad is idle, closet is idle, refrigerator is idle. So every object that I see, you know, is described in my memory stream because I have to know that the desk or that is idle because if I want to lay down on my plan, I have to tell GPT later on to generate a plan for me. If GPT wants me to lay down in a bed, I have to know that the bed is idle. Okay. So this goes into my my memory stream. Okay. So every observation by the agent goes into the memory stream and also observations, including behaviors performed by agents themselves or behaviors by other agents or non-agent objects. So also if another agent does something, it goes into my memory stream. And if I do something, it goes into my memory stream. Okay. Now, what happens if we want to retrieve something, for example, if, um, we have an agent, for example, here is a Bella. Okay. That is a Bella. And we ask Isabella or some other agents, you know, agent on the street asked us Isabella. Hey, what are you looking forward to the most right now? Okay. Now, the question is, how do we answer this question? Where we have to get the relevant information out of Isabella's memory stream. Okay. So we have to retrieve everything that's important for GPT to answer this question because again, everything is steered and controlled by GPT in the end. But we have to give GPT the correct information in order to answer this question in a way that makes sense for Isabella. Okay. And now, how do we do that? We have a retrieval algorithm. We talked about with the Anthony for C emails, we talked about relevance. Okay. This is basically the course in similarity. Okay. I have my embeddings and again, I can embed the last question, generate a vector, a vector representation out of it and embedding. And each entry here, my memory stream, I also have an embedding, just the number in our vector of numbers that represents basically each entry here. So what I can do in order to determine the relevancy for each entry, my memory stream, into this question is course in similarity on the embeddings. And this is the relevance here. Okay. This is what you read here, cross-in similarity, we saw that before. But what they have on top of that is something called recency. Well, that is easy wherever time stands, right? So of course, you know, observations that are more recent are maybe more interesting. Okay. And our relevant to answer this question. So recency is another factor that we take into account in order to determine what to retrieve or not to retrieve. Okay. It should be somewhat recent. And then the third one here is interesting. It's important. And importance is basically how to determine if something, if an entry here in the memory stream is actually important or not. Okay. And here again, we can use AI in order to determine importance because I don't want to, you know, manually for each entry here, the term and if it's important or not. Okay. And so for every entry in this memory stream, we trigger another prompt to GPT to determine the importance of that entry in our memory stream. So the prompt looks like the following. On the scale of one to ten, where one is purely mundane, for example, the brushing teeth or making bad and ten is extremely poor, no, I don't know if you pronounce that word. EG, a break up, as for example, break up or college acceptance, rate the likely portnals of the following piece of memory, buying groceries at the will market, pharmacy, rating, fill it. Okay. So we from basically GPT to determine the importance of every single entry in our memory stream. And we do it with this prompt that's generic for every, that's the same prompt for every single entry, except here this one here, the memory itself. That's for example, the first one was B desk is either would be probably something, you know, with a one. Okay. And then again, college break up would be a ten, you know, and they I, GPT is smart enough to determine, you know, based on those two examples, you know, what's important and what's not important. So but with this idea and this is really prompt engineering at its best, you know, the Stanford team came up with a third retrieval mechanism that we can use in order to get better retrievals. Okay. And this is a very big topic for many companies right now. How do I make sure when I have my private data, my private chatbot that the correct stuff for my huge databases get into the context. So how do I make sure that the retrieval algorithm works good and this retrieval is really the crucial part of making the successful GP chat GPT is good, but it's only as good as prompt prompt and this is why retrieval is very, very important. And so here the team of Stanford came up with a with an improvement of the retrieval algorithm by adding importance by using a skill or capability of of chat GPT or AI. And that's a pretty nice idea. It's generic idea that everybody in every company  can use now, adding this importance feature to your tree tree flag. Of course, you need a lot of cards as you can man. Open AI. So if you know, companies, you know, are willing to pay for it. But for every entry, every 10 seconds, you know, when you perceive your area, you have to, you know, for every single object, you have to to trigger this prompt, okay, in order to get the importance, but you store the importance in your memory stream and you use it later for retrieval, okay, and the combination of those three, basically give your tree retrieval score. And now you can determine what context you should give to, to answer this question. Okay. So it's, it's, it's a very interesting, very, you know, I mean, very, very interesting concepts and, and you know, generic concepts also that many people will and can use in order to work with these systems in order to achieve things that they couldn't achieve before. Okay. Now, another quite interesting thing here is reflection. So what they do here is basically, if you think about perception, you always just observe what people do, that's a nice thing. But you want to have in your memory stream, something that's a little bit more important in order to be retrieved later on. You don't want to have all the small things that that that compose maybe something important, but you want to have the important thing. Okay. So here they make an example of Claude Smller. And then we ask him if you have to choose one person or those you know, to spend an hour with who would it be, okay. And now based on the observations in the beginning, the system gave back that that Claude Smller wanted to be with Wolfgang. Okay, Wolfgang is his college dorm neighbor. They have they are completely they don't they don't have any deep interactions. Okay, they just meet you know, and say hi and meet and quite frequently they do now. And now, and they they realize, well, there's actually Maria Maria, someone who's doing very similar research like Klaus and they talked about a paper in this world and it was very interesting conversation because they were you know, very compelled about what they learned from each other. So it would be better that actually Klaus chooses Maria not and not Wolfgang. But in order to achieve that, all these lives they have to sum up. And this is basically a tree that you see here where you reflect on on a set of observations in order to generate a high level summary or or high level inside out of that. Okay, and of those insights again, you generate even higher insights into Klaus and what is what is preference is, for example, okay. So he on top of you see Klaus Smller is highly dedicated to research. So that's maybe something he realized, see the observations are reflecting, you know, so something we would also do, you know, in the evening, when we lie in bed and think about the day maybe we don't think about, you know, that exactly a three a.m. we went to the cafeteria in order to get a coffee, lucky, you know, but we think about the important points and store this in our, you know, in our brain in order to retrieve it later on because it was important. Okay. And now this is basically how they do it to generate these higher level reflections and put it into, you know, we've stream. I always have to move my windows here now. So this is how they implemented it. They took the memory stream and they decided in the first step, you initiate a reflection whenever the sum of importance scores for the latest events perceived by the agent exceeds a threshold of in this example, 150. Okay. So basically your agent lives and whenever, you know, all important scores for all the observations that were generated by by GPT exceeds 150, the agent steps back and says, well, I reflect now. Okay. And how does it do it? Well, in second step, you get the most 100 most recent records in agents memory stream. Okay. And now you put it into a prompt. Third step is the prompt not to reflect but approach to determine what to reflect on. Okay. So this is the prompt, you're given only the information above what are both salient high level questions. We can answer about the subjects in the statement. Okay. And in the context, we put basically the hundred most recent records in our memory stream. This is what we put in the context. Now we had GPT to say to tell us what what we actually should reflect on because so this is the prompt for that for the reflection part. And now it says, what topic is one thing you could reflect on is what topic is clause, more passionate about the second one is what is the relationship between class and Maria Lopez? Okay. So GPT generates questions. Those questions are used one by one. Okay, we take the first question. What the topic is clause, Mila, passionate about this question in order to retrieve from the memory insights relevant statements. Okay, this we what we see here again retrieval, we just talked about how we do it, reasons see importance and relevancy. This is how we get statements that are important in order to answer this first question. What is clause Mila passion about? Okay, then we asked based on those observant of those generated memories, we ask chat GPT again, what file high level insights can you infer from the statements above? Okay, and now chat GPT spits out what clause Mila is dedicated to his research on gentrification. Okay, and this we put in our memory stream again, that's a high level reflection. And again, we see here a mechanism Stanford did not just say, okay, just ask what you know, ask for reflections getting the 100 most recent observations. They know that's a good because GPT tried to basically use all 100 and confuse different reflections. It's better to first generate a prompt to ask GPT what questions we could reflect on and then do the reflections. So it's basically two steps here, two two prompts, no, instead of one. And they realized that's much better way similar to the to the step by step thing. It's better to split it up first generate the questions from the most from the form your memory stream that acute answer and then answer those questions one by one. Okay, again, a very interesting insight in how you should steer the system in order to make sure it's not confusing, confusing things. Okay. Now, let's take a look at the time to very up a little bit. Okay, it's very similar with planning, very interesting how they do it, you know, they basically let GPT plan generate a plan, you know, based on the agent summary descriptions based on the summary of a previous day, this is the context, then they ask, okay, do the planning for this day, GPT generates a plan, puts it into a memory stream, then takes the first entry, breaks it down, you know, works on this new new music from composition from one to five, this is the prompt for composing, decomposing this task. Um, move my window again. Um, and now here, um, let me just, um, and now further decomposing the five minutes, sketch it. Okay, so we take this one, take a quick break. So that's part of the hour to take a quick break and recharge because, you know, this is something GPT has to, know, typically if you work on a music composition, your brain stone first, you know, it's all in GPT, then you should also take a break, you know, and now it tells you, okay, how do I take a break? Okay, grab a light snack. Now in our small bill, we have to figure out how do we actually get a light snack, okay, and small and this is basically what brings us to our front end again, how to basically make sure, um, this person here gets a light snack, and the lens that is actually able to grab a light snack because that's, that's what you should do at 4 p.m. Right? So we break it down, um, step by step, and then execute the corresponding actions based on the plan that we have all it throughout the day. Okay, and now here's an example of, of, of a prompt template for the planning. Again, they have these prompt templates. And here they give an example, okay, described subtersk and five meter increments. They, they give an example and this prompt template, this is not a prompt instance, it's a template. So they're given a full example of what ChatGPT should do and how it should do it, okay, and then it's filling with those variables, and now it's ChatGPT's time to do it. Okay, so it's a full example and, um, given and then here GPD take this example, do it, and then we're going to do it,  the same thing now for the following variables. So this is how they see the look at the prompt templates. And let me just, so there are more things like for example reaction, if I have a plan, a daily plan or hourly plan or a flat plan, and I see I can't execute it anymore because something changes, you know, because there are other agents and things can change. They have a module for reaction. They have a module for dialogue. So how do I talk to other agents? How do I recall what we discussed yesterday and so on? Okay. Updating agent summary descriptions. So even that the initial values that we provided for for June, June, June, January, what the name was, can also change over time. Maybe he's changing based on his observations, you know, he liked his neighbor. This is how we initialized him. But maybe they, you know, experience over the next day, they experienced things and creating high-level reflections, somehow it turns out I don't like him anymore, you know, and this can change. And so they they observed it, you know, after, after several weeks of this game going on, you know, how agents, agents basically were changing even, you know, based on their experiences. And this is basically the goal what they try to achieve. Can I actually generate a human world with all the things that we do that we perceive that we that we do plan and you know, based on what we what we learned and what we observed and so on. And this is how they achieved it. And then again, a lot of these concepts behind it, you can transfer to many other erics. And this is what what people are doing right now. Okay. They're more and more of these multi-agent frameworks that you see, you know, where people are using prompts or large language models to implement multi-agent frameworks. Also Microsoft just published AutoGen. AutoGen is also basically multi-agent framework that you can implement with AutoGen in order to have similar scenarios like this. And again, use cases that we're talking a second about, you know, it's there's also serious use cases. I encourage you also to take a look at the GitHub project. This is basically you can find here this is basically the front end here where you'll find an entire front end implementation. And this one here is the back end server. Okay. And there you find basically here the main model is persona. This is basically the agent. And here you have all the cognitive modules. Okay. So basically, conversed, execute, perceive, plan, reflect, retrieve. So these are basically the modules that we see. Okay. And you would see it's a lot of Python code in the end, you know, but you see there's also a lot of prompt templates they use. And you know, also here are the prompt templates, you know, for example, for the daily planning, you have different variants, how they basically how these how they how they make sure the daily plans are done. Okay. So it's very interesting to just see how you can implement something like like this. So I encourage you to just navigate a little bit here through the through the steps, you know, perception. So it's pretty pretty interesting. Okay. Now, what I want to talk to you about now is really what I'm looking you to work on over the next couple of weeks. Okay. And don't worry. I don't expect you to, you know, generate something like like something we just just saw. You know, but you can use as inspiration. You know, and you can use similar things like retrieval like reflection, like planning for other scenarios that might be easier to implement. Okay. And just one example I want to give you. And again, for me, it's not so important in order in the end that you that you complete some some some project that the project I'm about to present. But rather to dig deep into it and to make a lot of experiments how it works, you know, so I want to see that each team of you is going deep into this technology is is is is trying your hardest in order to achieve something to get an output, you know, that that you want. Okay. Again, similar to the to the we've seen so far. And you can start very easy, you know, and then when you start very easily, you you pretty early see in the game that that it's not so easy in the end. Okay. And I did as an example is a very simple example that you can see here, a seller buyer negotiation. Okay. So I have here script where I use open AI in order to simulate a very similar as a very simple seller buyer conversation. So I have one agent with seller who wants to sell a product and I have another agent who is a buyer who wants to buy a product. Okay. Now they're negotiating. And you can you you can think of it as a tool you're, you know, you're in the marketing business and then you want to you have your marketing staff and you want to train them to be better sellers. You know, basically training, you know, tool that helps you to sell that. Okay. So how would I implement this? It's it's no rocket science. Okay. It's just I use open AI here. Okay. I'm maybe you can solve it. So I implement I load my stuff here. And I use variables here, basically product. I just chose an iPhone and a 13 because it was still there in 2001. So I figured maybe GPs using something from from pre train pre train data. I use the price and I have a number of rounds of negotiation. How many rounds basically the seller and the agent the seller and the buyer can discuss about the price. Okay. And now what I do is basically I use the same similar prop templates like like Stanford. Okay. So I have a system description first where I describe GPT basically what I'm up to. Okay. I describe basically the role and the context. Okay. So I describe this is a simulation of negotiation between a salesperson and a customer. Okay. I have product of interest. This is filled by my variable later. I have the price. There are most number of rounds three rounds. There are most three rounds of messages between the salesperson and the customer. When there's still no agreement on the final price, there's no deal. The customer starts with his or her message. Okay. So that's basically my system prompt. And here with this script, I basically put in my variables. Okay. The product, the price, and the number of rounds. So I just this one here, my system prompt. Now the next prompt you can see is a pro 932. That's one just you're in your instruction so I can add basically different different roles. So I just give this one. And here you can see the prompt for my salesperson. Okay. So let's take a look at the salesperson. Here you can see my instruction for the salesperson is you negotiate hard and under no circumstances you're going to crease the price. However, you have to make sure that there's deal in the end. Okay. So before I didn't put in the stuff with a deal and what happened is you know they didn't decrease or increase prices so they stick to their position and nothing happened. Okay. That was not the one I wanted to have. So I told them okay in the end you have to make sure there's there's deal. Otherwise, you know, you don't make money. So this is basically the first prompt for the salesperson and I have a customer prompt. So that's the prompt for the customer. You can negotiate hard try to decrease the price. There's no way you're going to buy the product for original price. However, you have to make sure that there's a deal in the end. Okay. So this is my customer. So here now I glue just all the prompts together. Okay. My salesperson prompt is now my person prompt where we replace the salesperson. My instruction for the salesperson is added and the same is done for the customer and now here I basically add my system prompt. Okay. So if I do this my salesperson prompt looks like this. Okay. So first the description to share GBT, the product, the price, there's three rounds. The customer starts with her message. Your role saves person. Okay. Now the agent, the one agent is a saves person and now you you instruction. You negotiate hard blah blah blah. Okay. And I have a customer prompt. It looks like this pretty much similar but here your role is customer and your instructions you don't go down the price. And now and now this is all I need. Okay. I just have a while up here. Okay. Round by round. I just generate one open AI call for the for the customer because he starts with a prompt. Then I glue the output to the salesperson prompt and to the customer prompt because every know what's what was said before. Then was the question of one student before. So I have to take the output from the one and give it into the element. And then I call the saying for the  the same person. And I go to the next round, you customer person, same person. Okay. So if I run this message, for example, I get a result here. So that's basically my output, my answer from chat GPT. So the customer says, hi, I'm interested in version of the iPhone. However, the price of seems a bit high for me, can we discuss the price further? That's it. I also added something already as you can see here, a little bit more, only generate the next answer for the customer did that because I noticed after a couple of experiments, GPT compiled the entire conversation for I didn't want that because I wanted to have the option, you know, to decrease the price and increase the price. So I wanted to step by step and not the entire thing. You know, this is basically only generate one answer. Okay, do it step by step. And then after generating the answer, display the latest off of sales person and the latest off of customer. Here's the format last customer last person last of no customer. Okay. And then also rounds to go before customer leaves. So I did these things already after doing a couple of experiments, because I noticed they confused the prices, you know, suddenly the seller had a high price then before he was already down and then he increased the price anymore, it didn't make sense. So he confused the numbers. So I wanted to the system to keep track of what's the latest offer, not forget about it. So and again, I glue those together here, response, I execute the sales person now. And now here's the answer of the sales person. Hi there, thank you for your interest in the iPhone. I understand you're concerned about the price given high demand, exceptional features that I for an office is quite reasonable. So, you know, it's a good argument, you know, to increase it. And now what I did is and this is basically how I see you started. Okay, it's not a lot of heavy coding. Now it's some coding. It's it's it's it's prompt, basically definitions, you know, and you do experiments. And I stored those experiments here, a separate text files, you know, and you can see here, for example, this last experiment, you know, how it goes back and forth, you know, how they and the number of rounds and the last offer and you can see here, it's kind of growing in an language. But I think here they didn't do it and they didn't end up with a deal. So it was hard to me to actually make sure they get a deal. So I would have to improve my promise to some exchange. Okay. And then in the beginning was really bad, you so you can play around and you can you can learn, you know, how to how to to engineer a promise. And this is basically what I'm looking for in the first point. And again, you have two agents. And now the interesting part is what I found is a paper where someone did very similar, he's the link to the paper, where someone did something very similar, like I did. But the difference is that this person did not try a lot to to improve the promise like either, you know, formulating, you know, make sure explicitly, you know, think of step by step and so on. This person added a third agent, a third agent, that was a critic. So they did the same as I did. But then they gave the entire conversation to a critic. And they specified the critic as a role. So you have the role of a coach, okay, your code of say it's coach, and you analyze this description and the conversation of the state's person and give feedback how to be better. Okay. And then he ran the conversations again, and he did several rounds, okay, several rounds of seller buyer, no negotiation, critic feedback, using the critic feedback for the buyer and the seller having negotiated again. Okay. And this is pretty smart again, that you actually and you see more or more to that you have multi agent systems where each agent in your system has a certain role. Okay. And this role helps you to improve things. Okay. And this again, it's quite quite interesting, quite interesting concept that it works so much better than trying to winch the reformulate your promise, rather as certain roads that take this, you know, role of, of, you know, analyzing this conversation and making it better for the next round, let's see, let's get you to use it. Why should use it? Okay. All right. And again, this becomes clear also how you can use this AI from so many different things for giving feedback, you know, for improving things all by itself. In the end. Okay. Now, let's, we, I know I'm all with time, but I'm asking you to stay with me for another 10 minutes, I would say, okay, I would like to start to talk about the project I have in mind. It's about a project that built on the stuff which is discussed, okay. So one thing is project one, simulated social platform with generative agents. So you can imagine that the hard part in Stanford was a lot also with, you know, the 2D world with having interacting with all these objects, you know, having a memory stream, we have to describe every object, you know, if it's idle or not, and to have at, you know, the importance value for it and so on, that's a lot of stuff. But imagine you have a social media platform, you know, we don't have a world, you know, you don't, you don't have a perception here, okay. The social social media platform, Facebook, Instagram, whatever, you just have the text in the, maybe in the just, okay. So doing something similar, you know, like Stanford for social media platform, okay, which just text, be Twitter or be Facebook, you know, shouldn't be so hard anymore, okay, because you don't have a perception part, the front end updates and all that stuff is not there anymore. What you still have is reflection part, you know, the reflection part is very, very strong probably, the retrieval part, the planning part, I'm not sure. If you think about dating platforms, it's maybe something else. But the idea is basically you have a platform, you simulate a platform with a set 1000 agents, every agent is talking to every agent, okay, constantly. They are talking, you know, you don't talk, they talk, okay. So basically, Tinder, okay, you talk, your agent, your avatar talks to a thousand woman at the same time, or a few thousand, a few days, I mean, you know what I mean. So 2000 possible partners at the same time, okay. And now the agents give back summaries to users, okay, basically look, I had 10,000 conversations, look at those five, it's really interesting. We kind of have a match. And what you need, of course, is the first initial description of yourself, okay, what you are, you are, okay, you can upload your hobbies, your interests, your CV, your goals, you know, you can also update your chat history that you have so far so that the agents chatting similar to you, okay. But it's basically something you can, you can imagine, okay. And for each member every day, you do conversations, you go back to, you have repetitive conversations with a certain agents. So you have to, you know, get the start out of your memory stream that was discussed with that particular agent or partner. You have reflection, you know, basically learning something about other people, you know, way for the next conversation, you're pulling a higher reflection that you had based on your other thousand conversations, okay. You learn something and then that discourage you use it for your next conversation. So all these things, how, how you are your agent basically changes over time and how your relationship to other agents change over time can all be simulated in a very similar way. Okay. And this is something I can, I think, could be very interesting. And you can start with two agents, right? You start with two agents and see if you add a third agent or fourth agent, what happens. And again, you do the testing first, maybe with a Cheshire PT UI for free, okay, you just paste in your your your inputs and outputs and see what happens. Do the testing in the free version, you know, and they later on, you can do it programmatically, first with a with a low cost version and see how your cost accrue and see which direction to go, you can also show I love to, okay, how good is that maybe? Now, takes more time, but just to see is it actually for that type of work, is it is it enough? Maybe that's because it's not about right or wrong, it's about creative creativity also. So maybe Lama too is enough, you can do it with that. Okay. So that would be interesting to simulate something like that. And again, it's not, it's not too hard to do that. It's not, it's there's a lot of issues, but this is what I'm looking for, for you to tap into these issues and see what part of pop some. And quite frankly, I haven't done something like that either. You know, so I'm learning too. And so we're learning together. So use me, you know, we can see the show we want you found and we'll discuss. Okay. And again, for example, you could also evaluate like the goal would be to also get into deep conversations, you know, not high level ones, you know, talk about whether, but rather deep conversations, how to make sure that that it's deep conversations. Well, maybe I can evaluate  I can use ChatGPT in order to evaluate how deep it was, and ask GPT, how can I make it deeper? Things like that. So be creative in order to also use ChatGPT to solve those problems that you will very well face. Now, any questions? Okay. I have a question. Yeah. And the question is, I saw that there are some slides missing in the one you uploaded. Is it on purpose or is it like you would... No, no, it's not on purpose. I always change a little bit. So I upload everything you need. I would upload this slide tag again because I think that's a newer one. If you see something you're missing, I think someone talked me, was asking for the one slide for the data sign circle. I will upload that to... And yeah, because I saw the... Where you made recommendations how to prompt ChatGPT, like, what was it? Yeah, oh yeah, right. This is not in the... The other time I said, yeah, I will upload this one here and yeah, I will do that. Thank you. Just see your mjads. Thanks. Okay. Now, okay, next one. It's somewhat related. Again, so your digital agent, yeah, if you let it go, will be better, of course, the more information you have a talk has about you, okay? And that's more about the focus of project two. It's more like a digital twin of yourself, okay? And this is... If you look at those companies here, Quiver, Rewind, augment, these are all startups working on something similar already. Like, for example, Rewind here, they just came up with this little chain here. It's a microphone. So, you know, people are just wearing it and it's a very good microphone that basically everything, you know, where the entire day, it stores everything you said, everything you heard, you know, everything, basically, it stores throughout the day as an embedding in a vector database, okay? So, yeah, everything that's acoustic information around you is stored in a vector database that you can, where you can retrieve things whenever you need it, okay? And this is, of course, also pretty, it's somewhat scary because if you want that or not, but I mean, you can think about this, you know, connecting, basically, building connected to your email to Twitter, tooling, the internet, whatever you want, you know, personal files, again, you know, microphone transcripts, you know, I don't know if you saw those meta, smart glasses, Facebook just came up with Ray Ban glasses. They look like cool glasses, not these VR glasses, but real glasses. But it has a little camera. So they, you know, you can think about images and videos too, okay, that's tracking this. But rewind again, that's really putting it already in a vector database, everything you talk. So you can go back, that's pretty, so all in there already. Now, and of course, the question is, what do you do with it, you know? So for any scenario, if you check, if you ask any questions like a personal question, you know, you use your phone and say, chat to your T, can you book, I don't know, just crazy cool vacation for me next week, that's it. And you can just say, and you know me, right? And that's it, right? So you want the AI actually to know you, you know, the better the AI knows you, the better it can come up with an idea of it, that's really cool that you wouldn't even believe that, you know, that you would love so much, because it knows you, knows you made it better than yourself, and it knows all the offers that are out there, okay? So it is somewhat scary understand, and we don't, I don't talk about ethics here, but ethics is a topic, no question, but for me, it's to make sure, you know, to have you see what the possibilities of this technology is, okay, and this is where we can go. And technically, it's not a big deal anymore. And again, if you implement something like this, you can do it locally also with ChromaDB, you know, you can use Lama too, so no, no, all your data is not leaving your laptop, you can implement something like this too. Quality, of course, will be better if you use OpenAI, but again, if you want to try around play with this space, you can do it also locally, you know. And again, here are some examples where people, you know, put something like, you know, like your eye where you can ask any questions, these are just like very basic questions, show me some photos of plans on my neighbor, what cities did I visit when I traveled to Japan, how many books did I purchase in April? These are very basic questions, but you can think also about, can you create a psychogram of myself, you know, what I might personality traits and, you know, things like this. And again, you know, please make some really cool vacation for me. I don't even know what I'm looking for, but you know, you know me and come up with a cool idea, you know, this is kind of where we can, can tap into. Okay, so yeah, explore in this space, create a digital twin of yourself and use it, you know, in certain scenarios that would be another kind of project. I could see it and you can also combine those, you know, I already want to, you know, thinking about basically combining one and two because for one, you need something from yourself either in order to make it compelling, so you can also easily combine those. But this is just for you, if you want to create this, you know, project one and you want to, you know, get one of those microphones, you know, maybe it helps to play around with both. Third one, project optimizer. This is a little bit related to my cellar buyer problem. This is similar to what the, you know, basically that you rate each round, you know, have a game, you know, I have a problem and you feel like okay, after doing it once, or you reform your prompts, do it again, it's not, it's not really perfect, but maybe better than a little bit than before. So you generate many examples of what you're trying to accomplish with different prompts. Now, if you have that, maybe you can use it as a, as a, as data, whereas context for chat, you can use the GPD to help you improving it. Okay, so basically, you, like in my example, put it, put it, put it, put it, put it, you know, first one was, was not so good, it was just one, second one was better, it was three and so on. And then you give all that into a context chat GPT and ask chat GPT, how can I generate prompts that, you know, so, so the number that you can see here, my examples is more, is closer to a 10 than a one. And I would be interested in what, what chat GPT generates, what kind of outputs it generates for, for, for similar prompts, and if it actually works better than, okay. So basically a prompt, based on chat GPT itself. Okay. And what you need is again, you do it a couple of times, with different, different prompts, you put in a rating and then you put the, the conversations plus rating into chat GPT and let chat GPT optimize it. Okay. And again, here are always links to the projects where people have tapped into similar issues, you know. So if you're, if you're interested in those, you know, read, read those papers too, they are somewhat going in the same direction. So there's, these are problems that, they're commonly arise now and I'll discuss in the scientific community. First one, simulate moderate multi person, person discussions. Also, I think very interesting if you're watching on a video, you know, shows or presidential debates or really, how can I maybe simulate something like this? You know, you mentioned you are Robert Haabek, you know, you're going into an interview show with this set of persons about this topic, you know. So can I maybe simulate this discussion? So you have this case six agents, you know, one agent is the moderator, make sure that you go back to the topic again. And again, you can't think of an AI moderator. Why would I need a real life moderator? And AI moderator can maybe much better. Okay. So implement an AI moderator, but also, you know, simulate a discussion like this. Is it realistic, you know. And of course, you have to describe each person in the beginning, you know, the background in order to see what happens. And each agent is able to discuss, I also have here, simulate the base with interruption. Interruption is a little bit harder because you can imagine when someone says something, you know, at any point in time and in those discussions, you could interrupt, right? In our multi-agent systems, it's not really possible. First, you know, you get the entire output and then you can say something. You can make it possible in your opening eye or in your chat by just taking from the output the first token, okay? Feed it into the other agent.  and generate prompt and say, you have the option to interrupt right now, the other agent. Do you want to interrupt? Yes or no? If not, then you wait for the second token. The second token give it back. You have the option to let CHGPT to decide if it wants to interrupt or not. So you can't simulate that. In the end, it would be lots of prompt calls, but you could also simulate that. It would be maybe interesting to see how it works. But the other thing is the danger is that it's really a high level discussion that's really not going into deeper aspects. So I could imagine that, for example, if you let CHGPT prepare for each person, so for example, Harvek, about the topic of climate change, let Harvek generate a so-called argument map, this link here to argdown, it's kind of a structure, how you structure arguments. Let GPD create an argument map for each participant, and use this argument map for every time the agent is asked if he wants to say something, use the argument map as a context. So CHGPT has already done death and can use it in order to argue something. Again, that's very interesting. There are six agents and they go back and forth and have decision, should I say something or not? That would be very interesting. There's also something called microfeedback. You see more and more tools that really feedback for every second. So again, you can add also here some, if you have one conversation, you can add an agent that gives a rating to each participant in this discussion. And give microfeedback, so per minute, in which minute did you perform not that good and why? And so on. So very interesting and opportunities, how to get something out. And also really, they're learning, how do I pitch something good or bad? If you have a topic, how should I pitch it, simulate those agents or respond? So also very interesting project, in my opinion, Project 5 recursively breakdown and executing complex tasks. So here the thing is, of course, an example we'll be writing a book. Is it possible right now to write a book also? Of course, Master thesis or Bachelor's thesis with CHGPT. And I invite you to explore that too. Okay. For your real thesis later on, you know, do what you think, when you write with me, we find a solution, you know, wait, wait, okay, to use CHGPT. But you will, there will still be a lot of other work too. But, but how would I do it? How would I use this system now to really generate a Bachelor thesis or a book or other things are games, applications, you know, also with code, with with the help of CHGPT, you know. Yeah, Christian? And again, you could imagine it's not spinning out the entire book. You know, it's not, it's not working that way. So you have to go back and forth and you have to maybe, you know, on a high level, start on a high level to generate a topic, a genre and a length of the book, the main characters, attributes, there's funnions and so on. And then let GPDT generate a detailed description for the characters, for the synopsis, then you have to break down the synopsis into chapters, the chapters into paragraphs. So here's the planning module, right? Breaking down things, but still making sure they fit together, okay? So, and also the review part, take another role, you know, add an agent that takes the role of a book critic to criticize, you know, the, you know, generate feedback per paragraph per chapter for the entire book. Let, let, let, let, CHGPT rework the pieces based on the feedback, okay? So it's really more, and you have to retrieve if you have a certain chapter, you want to retrieve relevant parts from the first rechapter that, that it's not completely out of scope, okay? So it's retrieval, it's planning, it's maybe also reflection, it's multi-agent system where you have a critic that, you know, that, that takes rounds, you know? So also very interesting components that, that, that you can glue together in order to see how far do you get with that, okay? With, with your version of seasons. And also, virtual thesis, remember, we have tools such as search APIs, right? So you can also include current information with, with a browser, okay? So in, in, in Langchain, for example, you can use the, the Google search API. So also that, you know, to, to generate something current, you need to, to tap into those tools, okay? Okay. Now let me speed up a little bit. We want to go. Yeah, okay. Project six exploring vector DB, okay? That's, that's really also very important right now because there are lots of people, and especially all the vector DB providers are thinking about what is a good way, how do I split my document, my email, my text into chunks? Because you want, in your vector DB, not the entire document, you want chunks, because you generate one vector of one thousand five hundred thirty six values, okay? And they represent either the entire meaning of a hundred page document, or the, the meaning of maybe two sentences, okay? And so if you choose it too large, you lose information in those vectors. And if you retrieve it later on, maybe this document is not retrieved because it talks about so much, so much uninteresting stuff. And the part that's important is very small. So it won't get the small part because the chunk is just too large. If it's too small, where you retrieve maybe this part because it's very, very relevant, based on the cost and similarity, but you don't, you lose information of what was before and after that chunk, okay? So there are many different ways how to deal with that. Already evolving strategies, how to chunk, how to store your data in a vector database, in order to have a better retrieval. You retrieve it algorithm, you can improve too, as did the Stanford guys, but also how to put it into a vector database. And for example, some people suggested, okay, you can always add a document summary to your document. You can add the placetical questions for each chunk. So for each chunk, you would ask GPT, what would be the placetical questions for this chunk to be answered later and then retrieve it? And then you store these questions that GPT provides for you, within this document too, as an embedded. So later on, the retrieval has already better idea how it can use this chunk. You can do recursive clustering, okay? That you drill down further in the end, it's vectors. You can cluster those vectors into talks. So I myself put all my emails once in a vector database and was just asking, okay, about, can you summarize all my emails about school? I was asking about my kids, we have lots of emails, and school related emails. And I want to just have a summary. So it took the ones, but it took also a lot of emails from my highs and the Horseshoe numbers, so from HGW, you put it in. And forgot good ones from school, because I only took the K, you know, 10 or 20 most relevant ones, because it doesn't fit more in the context. So it was two topics that it glued together and it was not good. So what if you do a recursive clustering? First cluster, you know, take lots of documents, do clustering, let the vector database with the help of CHGPT decide what kind of topics do you have here? You have school with your kids, but you also have school with your profession. So you have two themes, what do you want? And then have the user select a certain part, and you do it recursively, so you have some themes and so on. So here's a lot of space now where researchers tap into to really improve this entire storage and retrieval of vectors, okay, in a vector DB. And that's again, here, lots of papers where you see very similar ideas, you know, strategies how to do that. So I invite you to also play around with this one, you would have a large data set, you know, that you put in there, and then play around with, yeah, if you have some good goal or so, we can also discuss about emails, for example, just play around with that. Then also, in general, like an iDriven solution, you know, with the help of CHGPT, I had to mind something like, for example, Moodle. You can build on top of Moodle, of course, like an app that would allow me to use Moodle just by talking. Because if you look at Moodle, here's his link, every single transaction on Moodle, there's an AP call for it. Okay, so you can upload, programmatically, things on Moodle, you can download, you can look at things, you can put in groups, I can sign up students, I can look through Moodle, for each student what you look at, too, and that was actually new to me. So for each source on Moodle, then I have, I see who of you looked how many times at which source, okay, so I can download that.  that, just programmatically. So one use case for you would be, please submit documents X, Y for to do that assignment and provide some initial feedback. So you just use your phone, do that. And change the data. It's in the assignment. Looks to your submission already. Maybe give some feedback based on the assignment description. And when it's coded submitted for you, OK, for me, would be please extract all submissions from steering X, Y, summarize and give first impression on strengthen weaknesses. So also there, change of beauty reads everything and based on the experience gets already some first feedback for me about you. Technically again, no problem to make it happen. How good it works? Again, this is what I invite you to explore. And for building an app like either you can build a mood or a chat GPT plugin also. Or your own app, for example, here with Streamlit, it's very nice front-end, especially people use it for language model driven applications. So it's very easy to implement something this Streamlit is Python based. So it's very easy actually to implement an app like that, where you have some UI that can put chat, where you do some chatting and use it. So something like this to implement is very easy. So you can think about something like this for Moodle as a Moodle interface for you. And these Streamlit apps also run on your mobile phones. So you don't need to tie to just talk. So it's also possible. Last but not least, General Aidszans for developing software. So it is a paper, nice one, where someone really used also multi-agent system again, where they had different roles throughout the entire software development process. So they implemented basically the waterfall model with all the different role lines like CTO, CPU, product managers, the coding team, the testing team, the documentation team. And in each phase, they basically let GGPT generate the corresponding output. So here in Modality, the language, is also the output is code in the end. And let GGPT also review the code. It takes the role as a reviewer and do a code review. All can be done with GGPT in here. So I also invite you to implement something similar to this. Again, I think they implemented it with a front end again, which is quite boring because they're just sitting there. There was little other task. Again, if you leave that away, it has a certain complexity to make it really good. It's probably out of scope. But to get a first experience, to learn with that, that's what I'm looking for. And then like you again, this would be similar to the Stanford stuff. You just have, and to them, sell a buy. And then you have different roles, and you ask for the input. OK. So I'm very sorry to have taken a little bit too much time. But I think, yeah, also you're kind of a guinea pig. So I haven't done this sort of stuff before. And some methods also, these sort of projects, not. So it's really testing for me now, too. And you are the guinea pigs. And we'll see together how it works. I'm very curious about what comes out of it. And I'm looking forward to work with you. And again, let me know what you two, so for next week, it would be good if you could just let me know what's your favorite project, what you would like to work on, what your team is. And then I will already give some feedback. So we won't have any class next week. I will look through your stuff and either provide already some feedback via email or we'll be in two weeks on Monday in person. And also, if you have questions, again, always ask. We do it via email for the next two weeks. Again, send me your stuff. I will provide some feedback. And then from now on, it's really for you focusing on these projects for the rest of the semester, actually. And then the review sessions, again, this is where we meet. But again, if we want to meet outside, that's also possible. And you will put your code snippets on model as well, please. Right. The code I will put on model, the slides I will put on model. And we go from there. Are there any questions still? So I still have time. And don't worry about me. If you have questions, you can still take some time. Sorry. Yeah, do it. You do it. OK. Just about the recordings. Are they available for us? Like, I will make them available. Perfect. Thank you. Yeah, it's probably in Zoom Cloud. You have to probably look in with your HGW Zoom account. And then you should. OK, thank you. It's probably I will do it probably. You know, the leg, the leg just by tomorrow, it will happen. OK, thank you. Any other questions? I have a question regarding the other course, but we can do it later when there's no other questions to this course. Yeah. OK. I don't see anything in the chat. Yeah. Yeah, if there's nothing else, I can also ask now. OK, so if nobody has any more questions, then again, I hear from you next week and see you in two weeks. OK. All right, Mr. Beats. Yes. Yeah, German English. I don't want to. Yeah, I want to ask you for another course. They have already spoken a lot. I find the very interesting, I find generally their method from the teacher very interesting and he of course also. And then I wanted to ask if I can also share the other course. Yeah, my sister. My sister in this semester, or? Yeah, in this semester, because I don't have a student in the university, that's the office that can be signed. I don't need the theoretical for my student. But I'm interested in English. And I would like to have some of you in another course on Friday, every Friday, every two weeks. But I would try to get as many as possible. And just to share the details, to write it down. And then to see what they did there. Yeah. So the problem is that I also say, that's the time, that's the December, that's half a semester. And the main input is actually the following. Yeah. That means, if you come here now, we'll do a review session. That's what we're doing here. And then you won't be able to get too much of what you can offer. So that I can see you in the fashion course. Yeah. There I also have the recordings. They are not all from this semester, but a few are still from the Corona semester, where I had it now. Yeah. At the beginning, but not much has changed. So it starts with a new network, how it works, how it implement, and then I'm not in PyTorch. And so on. The last part of this transformer works. I've also brought it in. And then I've now recorded it on Friday, as recording. But that was the first time on the ATV. I've shown it to Zoom. How is the quality? I don't know. So I can switch it on Friday, but if it's really valuable, I can't even say it now. I haven't looked at it myself yet. But that's the whole part of the transformer, so it works with GPT. That's what it's all about. And as I said, I can switch it on Friday. I've never seen it again. How qualitative is it? Yeah, that's definitely better than nothing. Yeah. I can also ask you about the question if you're going to write a course in this case, where you're going to write notes, or if you say, then I'll just do the next semester. Exactly. For me, I'll do it again, so I don't know what to do next semester. But maybe I can do the next one with the next batch of work. But it's very well done. That's before the next batch of work. That's the, they have in the sixth semester. No, I'm in the other study. That's why I'm in the middle. I'm in the business of the junior business. I did a course in the company, and there I met the person who did the course. And he told me that he did a great thing, and then I wrote down. And he wanted to do the course, and I think it was very interesting. And I noticed that my knowledge, otherwise I wouldn't have been able to do it. I didn't know what to do. And now I know how to do it. And I'll know more about it. And that's the point. Okay. Well, in this case, I'll definitely go to the first page. Maybe just a short email, so I don't forget. I'll definitely go to the other course. There are the recordings. And that's what we have to work on. Otherwise, we could also go to a review session. I can't remember exactly what to do. 